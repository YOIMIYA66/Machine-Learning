# app.py
import sys
import os
import logging
import json
import uuid
import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from scipy import stats
import datetime
import traceback
import threading
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS

# å¯¼å…¥é…ç½®å’Œæ ¸å¿ƒåŠŸèƒ½æ¨¡å—
from config import KNOWLEDGE_BASE_DIR, AI_STUDIO_API_KEY
from rag_core import query_rag, initialize_rag_system, direct_query_llm
from ml_agents import query_ml_agent

# å¯¼å…¥å¢å¼ºç‰ˆRAGå’ŒMLé›†æˆåŠŸèƒ½
from rag_core_enhanced import enhanced_query_rag, enhanced_direct_query_llm
from ml_agents_enhanced import enhanced_query_ml_agent
from advanced_feature_analysis import integrate_ml_with_rag

# å¯¼å…¥å­¦ä¹ è·¯å¾„è§„åˆ’æ¨¡å—
from learning_planner import (
    generate_learning_path, 
    get_user_learning_path, 
    update_path_progress,
    predict_module_mastery, 
    predict_completion_time
)

# å¯¼å…¥æŠ€æœ¯å®éªŒå®¤æ¨¡å—
from tech_lab import (
    get_available_models, get_model_details, create_experiment,
    run_experiment, get_experiment, get_all_experiments
)

# Helper functions moved to the top
def is_rag_result_poor(query, rag_result):
    """
    è¯„ä¼°RAGç»“æœè´¨é‡æ˜¯å¦è¾ƒå·®

    è¯„ä¼°æŒ‡æ ‡:
    1. ç›¸å…³æ€§ - æ£€æŸ¥RAGçš„å›ç­”æ˜¯å¦ä¸é—®é¢˜ç›¸å…³
    2. ç¡®å®šæ€§ - æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åŒ…å«"æœªæ‰¾åˆ°"ã€"æ²¡æœ‰ç›¸å…³ä¿¡æ¯"ç­‰ä¸ç¡®å®šè¡¨è¿°
    3. ç½®ä¿¡åº¦ - æ£€æŸ¥æ–‡æ¡£æ£€ç´¢çš„åˆ†æ•°æ˜¯å¦è¿‡ä½
    """
    answer = rag_result.get("answer", "")

    # æ£€æŸ¥ä¸ç¡®å®šæ€§è¡¨è¾¾
    uncertainty_phrases = [
        "æ— æ³•æ‰¾åˆ°", "æ²¡æœ‰ç›¸å…³ä¿¡æ¯", "æœªèƒ½æ‰¾åˆ°", "æ— æ³•æä¾›",
        "æˆ‘ä¸çŸ¥é“", "æ— æ³•ç¡®å®š", "æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯",
        "To", "I cannot", "I don't", "Unable to"  # è‹±æ–‡å›ç­”ä¸­çš„ä¸ç¡®å®šæ€§è¡¨è¾¾
    ]

    if any(phrase in answer for phrase in uncertainty_phrases):
        return True

    # æ£€æŸ¥æ–‡æ¡£æ£€ç´¢åˆ†æ•°
    source_docs = rag_result.get("source_documents", [])
    if source_docs:
        # è·å–æœ€é«˜ç›¸å…³æ€§åˆ†æ•°
        max_score = max(
            [doc.get("score", 0) for doc in source_docs]
            if all("score" in doc for doc in source_docs)
            else [0]
        )
        # å¦‚æœæœ€é«˜åˆ†æ•°ä½äºé˜ˆå€¼ï¼Œè®¤ä¸ºç»“æœè´¨é‡è¾ƒå·®
        if max_score < 0.45:  # å¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é˜ˆå€¼
            return True

    # å¦‚æœå›ç­”è¿‡çŸ­æˆ–è¿‡é•¿ä¹Ÿå¯èƒ½è¡¨ç¤ºè´¨é‡é—®é¢˜
    if len(answer.strip()) < 30 or "The answer is" in answer:
        return True

    return False

# åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ± æ‰§è¡Œå™¨ç”¨äºå¼‚æ­¥ä»»åŠ¡
executor = ThreadPoolExecutor(max_workers=4)

app = Flask(__name__)  # Flaskä¼šè‡ªåŠ¨æŸ¥æ‰¾åŒçº§çš„ 'templates' æ–‡ä»¶å¤¹
CORS(app)

# --- æ—¥å¿—é…ç½® ---
# åŸºæœ¬é…ç½®ï¼Œç¡®ä¿åœ¨ app.run() ä¹‹å‰è®¾ç½®ï¼Œæˆ–è€…ç”± Flask çš„ debug æ¨¡å¼è‡ªåŠ¨å¤„ç†
# å¦‚æœä¸æ˜¯åœ¨debugæ¨¡å¼ä¸‹è¿è¡Œï¼Œæˆ–è€…éœ€è¦æ›´ç²¾ç»†çš„æ§åˆ¶ï¼Œå¯ä»¥å–æ¶ˆæ³¨é‡Šå¹¶è°ƒæ•´ä¸‹é¢çš„é…ç½®
# if not app.debug:
#     log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#     # To console
#     stream_handler = logging.StreamHandler()
#     stream_handler.setFormatter(log_formatter)
#     app.logger.addHandler(stream_handler)
#     # Optionally, to a file
#     # file_handler = logging.FileHandler('app.log')
#     # file_handler.setFormatter(log_formatter)
#     # app.logger.addHandler(file_handler)
#     app.logger.setLevel(logging.INFO)
# else:
#     # Debugæ¨¡å¼ä¸‹ï¼ŒFlaské€šå¸¸æœ‰è‡ªå·±çš„æ—¥å¿—å¤„ç†å™¨ï¼Œè¿™é‡Œç¡®ä¿çº§åˆ«
app.logger.setLevel(logging.INFO)
# -----------------

@app.route('/')
def index():
    """æ¸²æŸ“ä¸»HTMLé¡µé¢ã€‚"""
    return render_template('index.html')

@app.route('/query', methods=['POST'])
def query_endpoint():
    """å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ç«¯ç‚¹"""
    try:
        data = request.json
        query = data.get('query', '')
        mode = data.get('mode', 'data_analysis')
        
        if not query:
            return jsonify({"error": "è¯·æä¾›æŸ¥è¯¢æ–‡æœ¬"}), 400

        app.logger.info(f"æ¥æ”¶åˆ°æŸ¥è¯¢è¯·æ±‚: {query}, æ¨¡å¼: {mode}")

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å­¦ä¹ è·¯å¾„åˆ›å»ºæ„å›¾
        learning_path_keywords = ['å­¦ä¹ è·¯å¾„', 'å­¦ä¹ è®¡åˆ’', 'åˆ¶å®š', 'è§„åˆ’', 'æŒ‡å¯¼', 'å­¦ä¹ å»ºè®®']
        is_learning_path_query = any(keyword in query for keyword in learning_path_keywords)
        
        # å¦‚æœæ˜¯å­¦ä¹ è·¯å¾„ç›¸å…³æŸ¥è¯¢ï¼Œå°è¯•åˆ›å»ºå­¦ä¹ è·¯å¾„
        if is_learning_path_query and mode == 'data_analysis':
            try:
                app.logger.info("æ£€æµ‹åˆ°å­¦ä¹ è·¯å¾„åˆ›å»ºè¯·æ±‚")
                
                # ç®€å•è§£æç”¨æˆ·è¾“å…¥ï¼Œæå–å­¦ä¹ ç›®æ ‡å’Œä¿¡æ¯
                goal = query
                prior_knowledge = []
                weekly_hours = 10  # é»˜è®¤å€¼
                
                # å°è¯•ä»æŸ¥è¯¢ä¸­æå–æ›´å…·ä½“çš„ä¿¡æ¯
                import re
                hours_match = re.search(r'(\d+)\s*å°æ—¶', query)
                if hours_match:
                    weekly_hours = int(hours_match.group(1))
                
                if 'æ²¡æœ‰' in query or 'é›¶åŸºç¡€' in query or 'æ–°æ‰‹' in query:
                    prior_knowledge = []
                elif 'åŸºç¡€' in query:
                    prior_knowledge = ['ml_intro']
                
                # åˆ›å»ºå­¦ä¹ è·¯å¾„
                learning_path = generate_learning_path(
                    user_id='default_user',
                    goal=goal,
                    prior_knowledge=prior_knowledge,
                    weekly_hours=weekly_hours
                )
                
                # ç”Ÿæˆå­¦ä¹ è·¯å¾„æè¿°
                path_description = f"""
# ğŸ¯ æ‚¨çš„ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„å·²åˆ›å»º

## å­¦ä¹ ç›®æ ‡
{goal}

## è·¯å¾„æ¦‚è§ˆ
- **æ€»å…±æ¨¡å—æ•°**: {learning_path.get('total_modules', 0)}ä¸ª
- **é¢„è®¡æ€»å­¦ä¹ æ—¶é—´**: {learning_path.get('estimated_total_hours', 0)}å°æ—¶
- **é¢„è®¡å®Œæˆæ—¶é—´**: {learning_path.get('estimated_weeks', 0)}å‘¨ (æ¯å‘¨{weekly_hours}å°æ—¶)

## å­¦ä¹ æ¨¡å—é¢„è§ˆ
"""
                
                for i, module in enumerate(learning_path.get('modules', [])[:5], 1):
                    path_description += f"\n{i}. **{module.get('name', 'æœªå‘½åæ¨¡å—')}** - {module.get('estimated_hours', 0)}å°æ—¶\n   {module.get('description', 'æš‚æ— æè¿°')}\n"
                
                if len(learning_path.get('modules', [])) > 5:
                    path_description += f"\n... è¿˜æœ‰ {len(learning_path.get('modules', [])) - 5} ä¸ªæ¨¡å—\n"
                
                path_description += f"""
## ä¸‹ä¸€æ­¥
1. ç‚¹å‡»åˆ‡æ¢åˆ°"æˆ‘çš„è·¯å¾„"æ ‡ç­¾é¡µæŸ¥çœ‹å®Œæ•´å­¦ä¹ è·¯å¾„
2. å¼€å§‹ç¬¬ä¸€ä¸ªæ¨¡å—çš„å­¦ä¹ 
3. æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´æ¯å‘¨å­¦ä¹ æ—¶é—´

ç¥æ‚¨å­¦ä¹ æ„‰å¿«ï¼ğŸš€
"""
                
                return jsonify({
                    "answer": path_description,
                    "source_documents": [],
                    "is_ml_query": False,
                    "learning_path": {
                        "title": "ä¸ªæ€§åŒ–æœºå™¨å­¦ä¹ è·¯å¾„",
                        "content": path_description,
                        "path_id": learning_path.get('path_id'),
                        "total_modules": learning_path.get('total_modules', 0),
                        "estimated_hours": learning_path.get('estimated_total_hours', 0),
                        "weekly_hours": weekly_hours
                    },
                    "path_created": True
                })
            except Exception as e:
                app.logger.error(f"åˆ›å»ºå­¦ä¹ è·¯å¾„å¤±è´¥: {str(e)}")
                # ç»§ç»­ä½¿ç”¨æ™®é€šæŸ¥è¯¢å¤„ç†
        
        # å¤„ç†æ•°æ®åˆ†ææ¨¡å¼
        if mode == 'data_analysis':
            data_path = data.get('data_path')
            model_name = data.get('model_name')
            target_column = data.get('target_column')
            
            if not data_path or not model_name:
                # å¦‚æœæ²¡æœ‰æ•°æ®å’Œæ¨¡å‹ï¼Œç»™å‡ºæç¤º
                return jsonify({
                    "answer": "è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶å¹¶é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç„¶åå†æå‡ºæ‚¨çš„é—®é¢˜ã€‚æ‚¨å¯ä»¥ç‚¹å‡»ä¸Šä¼ æˆ–é€‰æ‹©æ•°æ®æŒ‰é’®å¼€å§‹ã€‚",
                    "source_documents": [],
                    "is_ml_query": False,
                    "needs_data_and_model": True
                })
            
            # æœºå™¨å­¦ä¹ ç›¸å…³æŸ¥è¯¢æ£€æµ‹
            ml_keywords = [
                'æœºå™¨å­¦ä¹ ', 'æ¨¡å‹', 'è®­ç»ƒ', 'é¢„æµ‹', 'åˆ†ç±»', 'å›å½’', 'èšç±»',
                'éšæœºæ£®æ—', 'å†³ç­–æ ‘', 'çº¿æ€§å›å½’', 'é€»è¾‘å›å½’', 'KNN', 'SVM',
                'æœ´ç´ è´å¶æ–¯', 'K-Means', 'æ•°æ®', 'ç‰¹å¾', 'å‡†ç¡®ç‡', 'MSE', 'RMSE'
            ]
            # æ“ä½œç±»å…³é”®è¯
            ml_ops_keywords = ['è®­ç»ƒ', 'é¢„æµ‹', 'æ¯”è¾ƒ', 'è¯„ä¼°', 'æ„å»º', 'è§£é‡Š', 'è‡ªåŠ¨', 'é›†æˆ', 'ç‰ˆæœ¬', 'åˆ†æ', 'æ¨è']

            is_ml_query = any(keyword.lower() in query.lower() for keyword in ml_keywords)
            is_ml_ops = any(op in query for op in ml_ops_keywords)

            # 1. æ“ä½œç±»é—®é¢˜ä¼˜å…ˆèµ°å¢å¼ºç‰ˆML Agent
            if is_ml_query and is_ml_ops:
                try:
                    app.logger.info("æ£€æµ‹åˆ°æœºå™¨å­¦ä¹ æ“ä½œç±»æŸ¥è¯¢ï¼Œä½¿ç”¨å¢å¼ºç‰ˆML Agentå¤„ç†")
                    result = enhanced_query_ml_agent(query, use_existing_model=True)
                    return jsonify(result)
                except Exception as e:
                    app.logger.error(f"å¢å¼ºç‰ˆML Agentå¤„ç†æ—¶å‡ºé”™ï¼Œå›é€€åˆ°RAG: {str(e)}")
                    # å°è¯•ä½¿ç”¨æ ‡å‡†ML Agent
                    try:
                        app.logger.info("å°è¯•ä½¿ç”¨æ ‡å‡†ML Agentå¤„ç†")
                        result = query_ml_agent(query)
                        return jsonify(result)
                    except Exception as e2:
                        app.logger.error(f"æ ‡å‡†ML Agentå¤„ç†æ—¶å‡ºé”™ï¼Œå›é€€åˆ°RAG: {str(e2)}")
                        # æœºå™¨å­¦ä¹ å¤„ç†å¤±è´¥æ—¶å›é€€åˆ°RAGç³»ç»Ÿ

        # å¤„ç†é€šç”¨å¤§æ¨¡å‹é—®ç­”æ¨¡å¼
        if mode == 'general_llm':
            app.logger.info("æ£€æµ‹åˆ°é€šç”¨å¤§æ¨¡å‹å›ç­”æ¨¡å¼ï¼Œç›´æ¥è°ƒç”¨LLM API")
            try:
                direct_llm_response = enhanced_direct_query_llm(query)
                return jsonify({
                    "answer": direct_llm_response.get("answer", "æœªèƒ½è·å–å›ç­”ã€‚"),
                    "source_documents": direct_llm_response.get("source_documents", []),
                    "is_ml_query": False,
                    "is_direct_answer": True,
                    "model_used": direct_llm_response.get("model_name", "General LLM (Enhanced)")
                })
            except Exception as e_enhanced_llm:
                app.logger.error(f"å¢å¼ºç‰ˆé€šç”¨å¤§æ¨¡å‹LLMè°ƒç”¨å¤±è´¥: {str(e_enhanced_llm)}ï¼Œå°è¯•æ ‡å‡†LLM", exc_info=True)
                try:
                    direct_llm_response = direct_query_llm(query)
                    return jsonify({
                        "answer": direct_llm_response.get("answer", "æœªèƒ½è·å–å›ç­”ã€‚"),
                        "source_documents": direct_llm_response.get("source_documents", []),
                        "is_ml_query": False,
                        "is_direct_answer": True,
                        "model_used": "General LLM (Standard)"
                    })
                except Exception as e_standard_llm:
                    app.logger.error(f"æ ‡å‡†é€šç”¨å¤§æ¨¡å‹LLMè°ƒç”¨ä¹Ÿå¤±è´¥: {str(e_standard_llm)}", exc_info=True)
                    return jsonify({"error": f"é€šç”¨å¤§æ¨¡å‹å¤„ç†æ—¶å‡ºé”™: {str(e_standard_llm)}"}), 500

        # 2. ä¸“ä¸šçŸ¥è¯†é—®ç­”ä¼˜å…ˆèµ°å¢å¼ºç‰ˆRAG
        app.logger.info("ä½¿ç”¨å¢å¼ºç‰ˆRAGç³»ç»Ÿå¤„ç†å¸¸è§„/çŸ¥è¯†ç±»æŸ¥è¯¢")
        try:
            # å°è¯•ä½¿ç”¨å¢å¼ºç‰ˆRAGå¤„ç†
            result = enhanced_query_rag(query)
        except Exception as e:
            app.logger.warning(f"å¢å¼ºç‰ˆRAGå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†RAG: {str(e)}")
            # å›é€€åˆ°æ ‡å‡†RAG
            result = query_rag(query)
            
        # 3. RAGæ•ˆæœä¸ä½³æ—¶å…œåº•å¢å¼ºç‰ˆLLM
        if is_rag_result_poor(query, result):
            app.logger.info("RAGç»“æœè´¨é‡ä¸ä½³ï¼Œåˆ‡æ¢åˆ°ç›´æ¥å¤§æ¨¡å‹å›ç­”")
            try:
                direct_llm_response = enhanced_direct_query_llm(query)
            except Exception as e:
                app.logger.warning(f"å¢å¼ºç‰ˆLLMå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†LLM: {str(e)}")
                direct_llm_response = direct_query_llm(query)
                
            result["answer"] = direct_llm_response["answer"]
            result["is_direct_answer"] = direct_llm_response.get("is_direct_answer", True)
        return jsonify(result)
    except Exception as e:
        app.logger.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}), 500

@app.route('/api/models/ml_models', methods=['GET'])
def get_ml_models():
    """
    è·å–ml_modelsç›®å½•ä¸­çš„æ¨¡å‹åˆ—è¡¨
    
    è¿”å›:
        JSONæ ¼å¼çš„æ¨¡å‹åˆ—è¡¨
    """
    try:
        model_dir = os.path.join(os.path.dirname(__file__), 'ml_models')
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
            return jsonify({"models": [], "message": "ml_modelsç›®å½•å·²åˆ›å»º"})
            
        # è·å–æ‰€æœ‰æ¨¡å‹æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼
        model_files = [f for f in os.listdir(model_dir) 
                      if f.endswith(('.pkl', '.joblib', '.h5', '.keras')) 
                      and os.path.isfile(os.path.join(model_dir, f))]
        
        # æå–æ¨¡å‹åç§°(å»æ‰æ‰©å±•å)
        model_names = [os.path.splitext(f)[0] for f in model_files]
        
        # æ·»åŠ æ¨¡å‹æè¿°ä¿¡æ¯
        models_info = [
            {
                "name": name,
                "path": os.path.join(model_dir, f),
                "size": os.path.getsize(os.path.join(model_dir, f)),
                "last_modified": os.path.getmtime(os.path.join(model_dir, f))
            } 
            for name, f in zip(model_names, model_files)
        ]
        
        return jsonify({"models": models_info})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/chat', methods=['POST'])
def chat_endpoint():
    """å¤„ç†èŠå¤©è¯·æ±‚çš„APIç«¯ç‚¹ã€‚"""
    data = request.get_json()
    if not data or 'query' not in data:
        app.logger.warning("APIè¯·æ±‚ç¼ºå°‘ 'query' å­—æ®µã€‚è¯·æ±‚ä½“: %s", data)
        return jsonify({"error": "è¯·æ±‚ä½“ä¸­ç¼ºå°‘ 'query' å­—æ®µ"}), 400

    user_query = data.get('query') # ä½¿ç”¨ .get() æ›´å®‰å…¨
    use_existing_model = data.get('use_existing_model', True) # é»˜è®¤ä¸ºTrueï¼Œä¼˜å…ˆä½¿ç”¨ç°æœ‰æ¨¡å‹
    if not isinstance(user_query, str) or not user_query.strip():
        app.logger.warning(f"APIæ¥æ”¶åˆ°æ— æ•ˆæŸ¥è¯¢: '{user_query}' (ç±»å‹: {type(user_query)})")
        return jsonify({"error": "æŸ¥è¯¢å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²"}), 400

    app.logger.info(f"APIæ¥æ”¶åˆ°æŸ¥è¯¢: '{user_query}'")
    ml_keywords = ['æœºå™¨å­¦ä¹ ', 'æ¨¡å‹', 'è®­ç»ƒ', 'é¢„æµ‹', 'å›å½’', 'åˆ†ç±»', 'ML', 'å†³ç­–æ ‘', 'éšæœºæ£®æ—',
                   'çº¿æ€§å›å½’', 'é€»è¾‘å›å½’', 'æ•°æ®åˆ†æ', 'ç‰¹å¾', 'æƒé‡', 'å‚æ•°', 'å‡†ç¡®ç‡', 'accuracy',
                   'precision', 'recall']
    ml_ops_keywords = ['è®­ç»ƒ', 'é¢„æµ‹', 'æ¯”è¾ƒ', 'è¯„ä¼°', 'æ„å»º', 'è§£é‡Š', 'è‡ªåŠ¨', 'é›†æˆ', 'ç‰ˆæœ¬', 'åˆ†æ', 'æ¨è']
    is_ml_query = any(keyword in user_query for keyword in ml_keywords)
    is_ml_ops = any(op in user_query for op in ml_ops_keywords)
    try:
        # ä¼˜å…ˆå¤„ç†é€šç”¨å¤§æ¨¡å‹å›ç­”æ¨¡å¼
        # å‰ç«¯å®é™…ä¼ é€’çš„é€šç”¨å¤§æ¨¡å‹æ¨¡å¼çš„ mode å€¼ä¸º 'general_llm'
        if data.get('mode') == 'general_llm': 
            app.logger.info("æ£€æµ‹åˆ°é€šç”¨å¤§æ¨¡å‹å›ç­”æ¨¡å¼ï¼Œç›´æ¥è°ƒç”¨LLM API")
            try:
                direct_llm_response = enhanced_direct_query_llm(user_query)
                return jsonify({
                    "answer": direct_llm_response.get("answer", "æœªèƒ½è·å–å›ç­”ã€‚"),
                    "source_documents": direct_llm_response.get("source_documents", []),
                    "is_ml_query": False,
                    "is_direct_answer": True,
                    "model_used": direct_llm_response.get("model_name", "General LLM (Enhanced)")
                })
            except Exception as e_enhanced_llm:
                app.logger.error(f"å¢å¼ºç‰ˆé€šç”¨å¤§æ¨¡å‹LLMè°ƒç”¨å¤±è´¥: {str(e_enhanced_llm)}ï¼Œå°è¯•æ ‡å‡†LLM", exc_info=True)
                try:
                    direct_llm_response = direct_query_llm(user_query)
                    return jsonify({
                        "answer": direct_llm_response.get("answer", "æœªèƒ½è·å–å›ç­”ã€‚"),
                        "source_documents": direct_llm_response.get("source_documents", []),
                        "is_ml_query": False,
                        "is_direct_answer": True,
                        "model_used": "General LLM (Standard)"
                    })
                except Exception as e_standard_llm:
                    app.logger.error(f"æ ‡å‡†é€šç”¨å¤§æ¨¡å‹LLMè°ƒç”¨ä¹Ÿå¤±è´¥: {str(e_standard_llm)}", exc_info=True)
                    return jsonify({"error": f"é€šç”¨å¤§æ¨¡å‹å¤„ç†æ—¶å‡ºé”™: {str(e_standard_llm)}"}), 500
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•™ç¨‹ç”Ÿæˆè¯·æ±‚
        elif (data.get('mode') == 'data_analysis' and
            data.get('data_preview') and
            data.get('model_name') and
            data.get('target_column')):
            
            app.logger.info(f"æ£€æµ‹åˆ°æ•™ç¨‹ç”Ÿæˆè¯·æ±‚: æ¨¡å‹ '{data.get('model_name')}', ç›®æ ‡åˆ— '{data.get('target_column')}'")
            llm_ml_context = {
                'data_preview': data.get('data_preview'),
                'model_name': data.get('model_name'),
                'target_column': data.get('target_column'),
                'generate_tutorial': True
            }
            
            try:
                # user_query ä¹Ÿä¼ é€’ç»™LLMï¼Œä»¥ä¾¿å®ƒäº†è§£ç”¨æˆ·çš„åŸå§‹æ„å›¾
                direct_llm_response = enhanced_direct_query_llm(user_query, llm_ml_context)
                return jsonify({
                    "answer": direct_llm_response.get("answer", "æœªèƒ½ç”Ÿæˆæ•™ç¨‹å†…å®¹ã€‚"),
                    "source_documents": [], 
                    "is_ml_query": True, 
                    "is_tutorial": True, 
                    "ml_model_used": data.get('model_name')
                })
            except Exception as e:
                app.logger.error(f"æ•™ç¨‹ç”ŸæˆLLMè°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
                return jsonify({"error": f"ç”Ÿæˆæ•™ç¨‹æ—¶å‡ºé”™: {str(e)}"}), 500
        
        elif is_ml_query and is_ml_ops:
            app.logger.info(f"æ£€æµ‹åˆ°æœºå™¨å­¦ä¹ æ“ä½œç±»æŸ¥è¯¢ï¼Œå°†ä½¿ç”¨å¢å¼ºç‰ˆML Agentå¤„ç†")
            try:
                # å°è¯•ä½¿ç”¨å¢å¼ºç‰ˆMLä»£ç†
                result = enhanced_query_ml_agent(user_query, use_existing_model=use_existing_model)
            except Exception as e:
                app.logger.warning(f"å¢å¼ºç‰ˆMLä»£ç†å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†MLä»£ç†: {str(e)}")
                # å›é€€åˆ°æ ‡å‡†MLä»£ç†
                result = query_ml_agent(user_query, use_existing_model=use_existing_model)
            
            # è¿”å›ç»“æœï¼Œä¿ç•™ç‰¹å¾åˆ†ææ•°æ®å’Œé¢„æµ‹ç»“æœ
            response_data = {
                "answer": result["answer"],
                "source_documents": [],
                "is_ml_query": True,
                "feature_analysis": result.get("feature_analysis", {}),
                "ml_model_used": result.get("model_used", "æœªçŸ¥æ¨¡å‹")
            }
            # å¦‚æœç»“æœä¸­åŒ…å«é¢„æµ‹ï¼Œæ·»åŠ åˆ°å“åº”ä¸­
            if "prediction" in result:
                response_data["prediction"] = result["prediction"]
            return jsonify(response_data)
        else:
            app.logger.info(f"ä½¿ç”¨å¢å¼ºç‰ˆRAGç³»ç»Ÿå¤„ç†å¸¸è§„/çŸ¥è¯†ç±»æŸ¥è¯¢")
            try:
                # å°è¯•ä½¿ç”¨å¢å¼ºç‰ˆRAGå¤„ç†ï¼Œå¯ç”¨æœºå™¨å­¦ä¹ é›†æˆ
                result = enhanced_query_rag(user_query, ml_integration=True)
            except Exception as e:
                app.logger.warning(f"å¢å¼ºç‰ˆRAGå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†RAG: {str(e)}")
                # å›é€€åˆ°æ ‡å‡†RAG
                result = query_rag(user_query)
                
            result["is_ml_query"] = False
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œæœºå™¨å­¦ä¹ é›†æˆ
            if is_ml_query and not is_ml_ops and "é¢„æµ‹" in user_query:
                app.logger.info("æ£€æµ‹åˆ°é¢„æµ‹ç±»æŸ¥è¯¢ï¼Œå°è¯•é›†æˆæœºå™¨å­¦ä¹ æ¨¡å‹ç»“æœ")
                try:
                    # æå–å¯èƒ½çš„é¢„æµ‹ç›®æ ‡å’Œç‰¹å¾
                    from rag_core_enhanced import extract_prediction_info, find_suitable_model, make_prediction_with_model
                    prediction_target, features = extract_prediction_info(user_query)
                    
                    if prediction_target and features:
                        # æŸ¥æ‰¾é€‚åˆè¯¥é¢„æµ‹ä»»åŠ¡çš„æ¨¡å‹
                        model_name = find_suitable_model(prediction_target)
                        
                        if model_name:
                            # åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹
                            model_result = make_prediction_with_model(model_name, features)
                            
                            # å°†æ¨¡å‹é¢„æµ‹ç»“æœä¸RAGç»“æœé›†æˆ
                            result = integrate_ml_with_rag(result, model_name, {
                                "prediction": model_result.get("predictions"),
                                "feature_importance": model_result.get("feature_importance", {}),
                                "model_metrics": model_result.get("metrics", {})
                            })
                except Exception as e:
                    app.logger.warning(f"æœºå™¨å­¦ä¹ é›†æˆå¤±è´¥: {str(e)}")
            
            # å¦‚æœRAGç»“æœè´¨é‡ä¸ä½³ï¼Œä½¿ç”¨å¢å¼ºç‰ˆLLM
            if is_rag_result_poor(user_query, result):
                app.logger.info("RAGç»“æœè´¨é‡ä¸ä½³ï¼Œåˆ‡æ¢åˆ°ç›´æ¥å¤§æ¨¡å‹å›ç­”")
                try:
                    # å¦‚æœæœ‰æœºå™¨å­¦ä¹ ç›¸å…³ä¿¡æ¯ï¼Œå°†å…¶ä¼ é€’ç»™å¢å¼ºç‰ˆLLM
                    ml_context = None
                    if result.get("ml_enhanced") or result.get("feature_analysis"):
                        ml_context = {
                            "model_name": result.get("ml_model_used", "æœªçŸ¥æ¨¡å‹"),
                            "prediction": result.get("prediction"),
                            "feature_importance": result.get("feature_analysis", {}).get("feature_importance", {}),
                            "model_metrics": result.get("model_metrics", {})
                        }
                    
                    direct_llm_response = enhanced_direct_query_llm(user_query, ml_context)
                except Exception as e:
                    app.logger.warning(f"å¢å¼ºç‰ˆLLMå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†LLM: {str(e)}")
                    direct_llm_response = direct_query_llm(user_query)
                    
                result["answer"] = direct_llm_response["answer"]
                result["is_direct_answer"] = direct_llm_response.get("is_direct_answer", True)
                result["ml_enhanced_llm"] = direct_llm_response.get("ml_enhanced", False)
            
            # å¦‚æœç»“æœä¸­åŒ…å«é¢„æµ‹ã€æ¨¡å‹æŒ‡æ ‡æˆ–ç‰¹å¾é‡è¦æ€§ï¼Œæ·»åŠ åˆ°å“åº”ä¸­
            if "prediction" in result:
                result["prediction"] = result["prediction"]
            if "model_metrics" in result:
                result["model_metrics"] = result["model_metrics"]
            if "feature_importance" in result:
                result["feature_importance"] = result["feature_importance"]

            return jsonify(result)
    except Exception as e:
        app.logger.error(f"/api/chat æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"}), 500

@app.route('/api/rebuild_vector_store', methods=['POST'])
def rebuild_vector_store_endpoint():
    """å¼ºåˆ¶é‡æ–°æ„å»ºå‘é‡æ•°æ®åº“çš„APIç«¯ç‚¹ã€‚"""
    app.logger.info("æ¥æ”¶åˆ°é‡å»ºå‘é‡æ•°æ®åº“çš„è¯·æ±‚ã€‚")
    try:
        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡é‡å»ºå‘é‡åº“
        executor.submit(initialize_rag_system, force_recreate_vs=True)
        app.logger.info("å‘é‡æ•°æ®åº“é‡å»ºæµç¨‹å·²å¼‚æ­¥å¯åŠ¨ã€‚")
        return jsonify({"message": "å‘é‡æ•°æ®åº“é‡å»ºæµç¨‹å·²å¼‚æ­¥å¯åŠ¨ï¼Œè¯·ç¨åæŸ¥è¯¢çŠ¶æ€ã€‚"}), 202
    except Exception as e:
        app.logger.error(f"/api/rebuild_vector_store æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"error": f"é‡å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}"}), 500

@app.route('/api/ml/train', methods=['POST'])
def train_model_endpoint():
    """è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„APIç«¯ç‚¹"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸ºç©º"}), 400

    required_fields = ['model_type', 'data_path', 'target_column']
    for field in required_fields:
        if field not in data:
            return jsonify({"error": f"ç¼ºå°‘å¿…è¦å­—æ®µ '{field}'"}), 400

    try:
        # åˆ›å»ºä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # å¤åˆ¶è¯·æ±‚æ•°æ®ä»¥ä¾¿å¼‚æ­¥ä»»åŠ¡ä½¿ç”¨
        task_data = data.copy()
        
        # å¼‚æ­¥æ‰§è¡Œæ¨¡å‹è®­ç»ƒ
        def async_train_model(task_id, task_data):
            try:
                from ml_models import train_model

                model_type = task_data['model_type']
                data_path = task_data['data_path']
                target_column = task_data['target_column']
                model_name = task_data.get('model_name')
                categorical_columns = task_data.get('categorical_columns', [])
                numerical_columns = task_data.get('numerical_columns', [])
                model_params = task_data.get('model_params', {})
                test_size = task_data.get('test_size', 0.2)
                
                app.logger.info(f"å¼€å§‹å¼‚æ­¥è®­ç»ƒä»»åŠ¡ {task_id}: {model_type} æ¨¡å‹ï¼Œç›®æ ‡åˆ—: {target_column}")
                
                # å¦‚æœæ˜¯Excelæ–‡ä»¶ï¼Œè½¬æ¢ä¸ºCSVä»¥ä¾¿æ›´å¥½åœ°å¤„ç†
                if data_path.endswith('.xlsx'):
                    import pandas as pd
                    df = pd.read_excel(data_path)
                    csv_path = data_path.replace('.xlsx', f'_processed_{task_id}.csv')
                    df.to_csv(csv_path, index=False)
                    data_path = csv_path
                
                # æ‰§è¡Œè®­ç»ƒ
                result = train_model(
                    model_type=model_type,
                    data=data_path,
                    target_column=target_column,
                    model_name=model_name,
                    categorical_columns=categorical_columns,
                    numerical_columns=numerical_columns,
                    model_params=model_params,
                    test_size=test_size
                )
                
                # å°†ä¸´æ—¶CSVæ¸…ç†æ‰
                if data_path.endswith(f'_processed_{task_id}.csv') and os.path.exists(data_path):
                    try:
                        os.remove(data_path)
                    except Exception as e:
                        app.logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {data_path} å¤±è´¥: {str(e)}")
                
                app.logger.info(f"å¼‚æ­¥è®­ç»ƒä»»åŠ¡ {task_id} å®Œæˆ: {result.get('model_name')}")
            except Exception as e:
                app.logger.error(f"å¼‚æ­¥è®­ç»ƒä»»åŠ¡ {task_id} å¤±è´¥: {str(e)}", exc_info=True)
        
        # æäº¤å¼‚æ­¥ä»»åŠ¡
        executor.submit(async_train_model, task_id, task_data)
        
        return jsonify({
            "message": f"æ¨¡å‹è®­ç»ƒä»»åŠ¡å·²å¼‚æ­¥å¯åŠ¨ (ID: {task_id})",
            "task_id": task_id,
            "status": "processing",
            "model_type": data['model_type'],
            "target_column": data['target_column']
        }), 202
    except Exception as e:
        app.logger.error(f"/api/ml/train æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"error": f"å¯åŠ¨è®­ç»ƒä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}), 500

@app.route('/api/ml/predict', methods=['POST'])
def predict_endpoint():
    """ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹çš„APIç«¯ç‚¹"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸ºç©º"}), 400

    required_fields = ['model_name', 'input_data']
    for field in required_fields:
        if field not in data:
            return jsonify({"error": f"ç¼ºå°‘å¿…è¦å­—æ®µ '{field}'"}), 400

    try:
        # ç›´æ¥è°ƒç”¨ml_models.pyä¸­çš„predictå‡½æ•°
        from ml_models import predict

        model_name = data['model_name']
        input_data = data['input_data']
        target_column = data.get('target_column')

        # è¿›è¡Œé¢„æµ‹
        result = predict(
            model_name=model_name,
            input_data=input_data,
            target_column=target_column
        )

        # æ ¼å¼åŒ–ç»“æœä»¥ä¾¿å‰ç«¯æ˜¾ç¤º
        formatted_result = {
            "model_name": result["model_name"],
            "predictions": result["predictions"],
            "input_data": result["input_data"]
        }

        # å¦‚æœæœ‰è¯„ä¼°æŒ‡æ ‡ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if "accuracy" in result:
            formatted_result["accuracy"] = result["accuracy"]
        if "mse" in result:
            formatted_result["mse"] = result["mse"]
            formatted_result["r2"] = result["r2"]

        return jsonify(formatted_result), 200
    except Exception as e:
        app.logger.error(f"/api/ml/predict æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"error": f"è¿›è¡Œé¢„æµ‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}), 500

@app.route('/api/ml/models', methods=['GET'])
def list_models_endpoint():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹çš„APIç«¯ç‚¹"""
    try:
        # ç›´æ¥è°ƒç”¨ml_models.pyä¸­çš„list_available_modelså‡½æ•°
        from ml_models import list_available_models, MODEL_CATEGORIES

        models = list_available_models()

        # æŒ‰ç±»åˆ«ç»„ç»‡æ¨¡å‹
        categorized_models = {}
        for category, model_types in MODEL_CATEGORIES.items():
            categorized_models[category] = []
            for model in models:
                if model["type"] in model_types:
                    categorized_models[category].append({
                        "name": model["name"],
                        "type": model["type"],
                        "path": model["path"]
                    })

        return jsonify({
            "models": models,
            "categorized_models": categorized_models,
            "total_count": len(models)
        }), 200
    except Exception as e:
        app.logger.error(f"/api/ml/models æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"error": f"åˆ—å‡ºæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}), 500

@app.route('/api/ml/upload', methods=['POST'])
def upload_data_endpoint():
    """ä¸Šä¼ æ•°æ®æ–‡ä»¶çš„APIç«¯ç‚¹"""
    if 'file' not in request.files:
        return jsonify({"error": "æ²¡æœ‰æ–‡ä»¶éƒ¨åˆ†"}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400
        
    # å®‰å…¨æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    allowed_extensions = ['.csv', '.xlsx', '.xls', '.json']
    file_ext = os.path.splitext(file.filename.lower())[1]
    if file_ext not in allowed_extensions:
        return jsonify({"error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚ä»…æ”¯æŒ {', '.join(allowed_extensions)}"}), 400

    try:
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        uploads_dir = os.path.join(os.getcwd(), 'uploads')
        os.makedirs(uploads_dir, exist_ok=True)

        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å (ä½¿ç”¨UUIDé¿å…æ–‡ä»¶åå†²çª)
        safe_filename = f"{str(uuid.uuid4())}{file_ext}"
        file_path = os.path.join(uploads_dir, safe_filename)
        
        # ä¿å­˜åŸå§‹æ–‡ä»¶åä¸å®‰å…¨æ–‡ä»¶åçš„æ˜ å°„å…³ç³»
        original_filename = file.filename
        
        # ä¿å­˜æ–‡ä»¶
        file.save(file_path)
        app.logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {original_filename} -> {file_path}")

        # è¯»å–æ•°æ®å¹¶å¤„ç†ä¸åŒæ ¼å¼
        df = None
        try:
            if file_ext == '.csv':
                df = pd.read_csv(file_path, keep_default_na=False, na_values=['NaN', 'N/A', 'NA', 'nan', 'null'])
            elif file_ext in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path, keep_default_na=False, na_values=['NaN', 'N/A', 'NA', 'nan', 'null'])
            elif file_ext == '.json':
                df = pd.read_json(file_path, orient='records')
                df = df.fillna('')
        except Exception as e:
            # æ¸…ç†å·²ä¸Šä¼ çš„æ–‡ä»¶
            if os.path.exists(file_path):
                os.remove(file_path)
            app.logger.error(f"è¯»å–æ–‡ä»¶ {original_filename} å¤±è´¥: {str(e)}")
            return jsonify({"error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"}), 400
            
        if df is None or df.empty:
            # æ¸…ç†å·²ä¸Šä¼ çš„æ–‡ä»¶
            if os.path.exists(file_path):
                os.remove(file_path)
            return jsonify({"error": "æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®"}), 400

        # æ¨æ–­æ¯åˆ—çš„æ•°æ®ç±»å‹
        column_types = {}
        categorical_columns = []
        numerical_columns = []

        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                if df[col].nunique() < min(10, len(df) // 10):  # å¦‚æœå”¯ä¸€å€¼è¾ƒå°‘ï¼Œä»è§†ä¸ºåˆ†ç±»
                    categorical_columns.append(col)
                    column_types[col] = 'categorical'
                else:
                    numerical_columns.append(col)
                    column_types[col] = 'numerical'
            else:
                categorical_columns.append(col)
                column_types[col] = 'categorical'

        # ä½¿ç”¨json_compatible_resultå¤„ç†ç»“æœï¼Œç¡®ä¿æ²¡æœ‰NaNå€¼
        result = json_compatible_result({
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
            "file_path": file_path,
            "original_filename": original_filename,
            "columns": df.columns.tolist(),
            "column_types": column_types,
            "categorical_columns": categorical_columns,
            "numerical_columns": numerical_columns,
            "preview": df.head(5).to_dict('records'),
            "row_count": len(df),
            "column_count": len(df.columns)
        })

        return jsonify(result), 200
    except Exception as e:
        app.logger.error(f"ä¸Šä¼ æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        return jsonify({"error": f"ä¸Šä¼ æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"}), 500

def json_compatible_result(data):
    """ç¡®ä¿æ•°æ®å¯ä»¥è¢«JSONåºåˆ—åŒ–"""
    if isinstance(data, dict):
        return {k: json_compatible_result(v) for k, v in data.items()}
    elif isinstance(data, list):
        return [json_compatible_result(item) for item in data]
    elif isinstance(data, (np.int64, np.int32, np.int16, np.int8)):
        return int(data)
    elif isinstance(data, (np.float64, np.float32, np.float16)):
        if np.isnan(data) or np.isinf(data):
            return None
        return float(data)
    elif pd and isinstance(data, pd.Series):
        return json_compatible_result(data.tolist())
    elif pd and isinstance(data, pd.DataFrame):
        return json_compatible_result(data.to_dict(orient='records'))
    elif pd and isinstance(data, pd.Timestamp):
        return data.isoformat()
    elif isinstance(data, np.ndarray):
        return json_compatible_result(data.tolist())
    elif isinstance(data, (datetime.datetime, datetime.date)):
        return data.isoformat()
    return data

@app.route('/api/ml/analyze', methods=['POST'])
def analyze_data_endpoint():
    """åˆ†ææ•°æ®é›†çš„APIç«¯ç‚¹"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸ºç©º"}), 400

    required_fields = ['data_path']
    for field in required_fields:
        if field not in data:
            return jsonify({"error": f"ç¼ºå°‘å¿…è¦å­—æ®µ '{field}'"}), 400

    try:
        # è¯»å–æ•°æ®æ–‡ä»¶
        data_path = data['data_path']
        target_column = data.get('target_column')

        # è¯»å–æ•°æ®ï¼Œå¤„ç†NaNå€¼
        if data_path.endswith('.csv'):
            df = pd.read_csv(data_path, keep_default_na=False, na_values=['NaN', 'N/A', 'NA', 'nan', 'null'])
        elif data_path.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(data_path, keep_default_na=False, na_values=['NaN', 'N/A', 'NA', 'nan', 'null'])
        elif data_path.endswith('.json'):
            # æ”¯æŒJSONæ–‡ä»¶
            df = pd.read_json(data_path)
        else:
            return jsonify({"error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒCSVã€Excelå’ŒJSON"}), 400

        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        basic_stats = {}
        for col in df.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                basic_stats[col] = {
                    "mean": float(df[col].mean()) if pd.notna(df[col].mean()) else None,
                    "median": float(df[col].median()) if pd.notna(df[col].median()) else None,
                    "std": float(df[col].std()) if pd.notna(df[col].std()) else None,
                    "min": float(df[col].min()) if pd.notna(df[col].min()) else None,
                    "max": float(df[col].max()) if pd.notna(df[col].max()) else None,
                    "missing": int(df[col].isna().sum())
                }
            else:
                value_counts = df[col].value_counts().to_dict()
                # å°†é”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç¡®ä¿JSONåºåˆ—åŒ–ä¸ä¼šå‡ºé”™
                value_counts = {str(k): int(v) for k, v in value_counts.items()}
                basic_stats[col] = {
                    "unique_values": int(df[col].nunique()),
                    "missing": int(df[col].isna().sum()),
                    "most_common": json.loads(json.dumps(value_counts)) # Ensure this is serializable
                }

        # ç›¸å…³æ€§åˆ†æï¼ˆä»…å¯¹æ•°å€¼åˆ—ï¼‰
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        correlation = None
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr().round(3)
            # Convert NaN/Inf in correlation matrix to None
            # Use json_compatible_result here too for safety
            correlation = json_compatible_result(corr_matrix.to_dict())

        # ç›®æ ‡åˆ—åˆ†æï¼ˆå¦‚æœæä¾›ï¼‰
        target_analysis = None
        if target_column and target_column in df.columns:
            if pd.api.types.is_numeric_dtype(df[target_column]):
                # å›å½’é—®é¢˜åˆ†æ
                target_analysis = {
                    "type": "regression",
                    "distribution": {
                        "mean": float(df[target_column].mean()) if pd.notna(df[target_column].mean()) else None,
                        "median": float(df[target_column].median()) if pd.notna(df[target_column].median()) else None,
                        "skewness": float(stats.skew(df[target_column].dropna())) if not np.isnan(stats.skew(df[target_column].dropna())) else None,
                        "kurtosis": float(stats.kurtosis(df[target_column].dropna())) if not np.isnan(stats.kurtosis(df[target_column].dropna())) else None
                    }
                }

                # è®¡ç®—ä¸ç›®æ ‡åˆ—çš„ç›¸å…³æ€§
                if len(numeric_cols) > 1:
                    target_corr = df[numeric_cols].corr()[target_column].drop(target_column).sort_values(ascending=False)
                    # Convert NaN/Inf in target correlation to None
                     # Use json_compatible_result here too for safety
                    target_analysis["correlations"] = json_compatible_result(target_corr.to_dict())
            else:
                # åˆ†ç±»é—®é¢˜åˆ†æ
                class_distribution = df[target_column].value_counts().to_dict()
                # Ensure keys and values are serializable
                class_distribution = {str(k): (int(v) if pd.notna(v) else None) for k, v in class_distribution.items()}
                target_analysis = {
                    "type": "classification",
                    "class_distribution": class_distribution,
                    "class_count": len(class_distribution)
                }

        # æ¨èæ¨¡å‹
        recommended_models = []
        if target_column and target_column in df.columns:
            if pd.api.types.is_numeric_dtype(df[target_column]):
                # å›å½’é—®é¢˜æ¨èæ¨¡å‹ï¼šçº¿æ€§å›å½’
                recommended_models = ["linear_regression"]
            elif df[target_column].nunique() > 0 and df[target_column].nunique() < len(df) * 0.5: # å‡è®¾åˆ†ç±»é—®é¢˜ç±»åˆ«æ•°å°äºæ€»æ ·æœ¬æ•°çš„ä¸€åŠ
                # åˆ†ç±»é—®é¢˜æ¨èæ¨¡å‹ï¼šé€»è¾‘å›å½’ã€K-è¿‘é‚»ã€å†³ç­–æ ‘ã€å‘é‡æœºã€æœ´ç´ è´å¶æ–¯
                recommended_models = ["logistic_regression", "knn_classifier", "decision_tree", "svm_classifier", "naive_bayes"]
            else:
                # å¦‚æœç›®æ ‡åˆ—æ˜¯å…¶ä»–ç±»å‹æˆ–ç±»åˆ«è¿‡å¤šï¼Œæš‚ä¸æ¨èç›‘ç£æ¨¡å‹
                recommended_models = []
        else:
            # æ— ç›‘ç£å­¦ä¹ æ¨èæ¨¡å‹ï¼šK-Means
            recommended_models = ["kmeans"]

        # Prepare the result dictionary
        analysis_result = {
            "row_count": len(df),
            "column_count": len(df.columns),
            "columns": df.columns.tolist(),
            "basic_stats": basic_stats,
            "correlation": correlation,
            "target_analysis": target_analysis,
            "recommended_models": recommended_models,
            "message": "æ•°æ®åˆ†æå®Œæˆ"
        }

        # Recursively convert NaN/Inf to None before sending
        return jsonify(json_compatible_result(analysis_result)), 200
    except Exception as e:
        app.logger.error(f"/api/ml/analyze æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        # Ensure error response is also JSON compatible
        return jsonify(json_compatible_result({"error": f"åˆ†ææ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"})), 500

@app.route('/api/ml/analyze', methods=['GET'])
def analyze_data_get_endpoint():
    """å¤„ç†GETè¯·æ±‚çš„æ•°æ®åˆ†æï¼Œç”¨äºæ¨¡å‹æ¯”è¾ƒç­‰åœºæ™¯"""
    try:
        file_path = request.args.get('file_path')
        if not file_path:
            return jsonify({"error": "æœªæä¾›æ–‡ä»¶è·¯å¾„å‚æ•°"}), 400
            
        # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(file_path):
            return jsonify({"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}), 404
            
        # è¯»å–å¹¶åˆ†ææ•°æ®
        df, error = load_dataframe(file_path)
        if error:
            return jsonify({"error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {error}"}), 400
            
        # è·å–åˆ—ä¿¡æ¯
        columns = df.columns.tolist()
        
        # ç®€å•åˆ†æ
        result = {
            "columns": columns,
            "row_count": len(df),
            "column_count": len(columns),
            "file_path": file_path
        }
        
        return jsonify(result)
    except Exception as e:
        traceback_str = traceback.format_exc()
        app.logger.error(f"åˆ†ææ•°æ®å¤±è´¥: {str(e)}\n{traceback_str}")
        return jsonify({"error": f"åˆ†ææ•°æ®å¤±è´¥: {str(e)}"}), 500

def load_dataframe(file_path):
    """
    åŠ è½½æ•°æ®æ–‡ä»¶åˆ°DataFrame
    
    æ”¯æŒçš„æ–‡ä»¶æ ¼å¼:
    - CSV (.csv)
    - Excel (.xlsx, .xls)
    - JSON (.json)
    
    Args:
        file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        
    Returns:
        tuple: (DataFrame, error_message)
        å¦‚æœæˆåŠŸåŠ è½½ï¼Œerror_messageä¸ºNone
        å¦‚æœåŠ è½½å¤±è´¥ï¼ŒDataFrameä¸ºNoneï¼Œerror_messageåŒ…å«é”™è¯¯ä¿¡æ¯
    """
    if not os.path.exists(file_path):
        return None, f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
    
    try:
        file_ext = os.path.splitext(file_path.lower())[1]
        
        # CSVæ–‡ä»¶å¤„ç†
        if file_ext == '.csv':
            # å°è¯•ä¸åŒçš„ç¼–ç 
            encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
            
            for encoding in encodings:
                try:
                    df = pd.read_csv(file_path, encoding=encoding, keep_default_na=False, 
                                     na_values=['NaN', 'N/A', 'NA', 'nan', 'null'])
                    return df, None
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    return None, f"è¯»å–CSVæ–‡ä»¶é”™è¯¯: {str(e)}"
            
            return None, "æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç æ ¼å¼è¯»å–CSVæ–‡ä»¶"
        
        # Excelæ–‡ä»¶å¤„ç†
        elif file_ext in ['.xlsx', '.xls']:
            try:
                df = pd.read_excel(file_path, keep_default_na=False,
                                  na_values=['NaN', 'N/A', 'NA', 'nan', 'null'])
                return df, None
            except Exception as e:
                return None, f"è¯»å–Excelæ–‡ä»¶é”™è¯¯: {str(e)}"
        
        # JSONæ–‡ä»¶å¤„ç†
        elif file_ext == '.json':
            try:
                df = pd.read_json(file_path)
                return df, None
            except Exception as e:
                return None, f"è¯»å–JSONæ–‡ä»¶é”™è¯¯: {str(e)}"
        
        else:
            return None, f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ï¼Œä»…æ”¯æŒCSVã€Excelå’ŒJSON"
    
    except Exception as e:
        return None, f"åŠ è½½æ•°æ®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

@app.route('/api/ml/model_versions', methods=['POST'])
def create_model_version_endpoint():
    """åˆ›å»ºæ¨¡å‹æ–°ç‰ˆæœ¬çš„APIç«¯ç‚¹"""
    return jsonify({"success": False, "error": "æ­¤åŠŸèƒ½å·²ç¦ç”¨"}), 404

@app.route('/api/ml/model_versions/<model_name>', methods=['GET'])
def get_model_versions_endpoint(model_name):
    """è·å–æ¨¡å‹æ‰€æœ‰ç‰ˆæœ¬çš„APIç«¯ç‚¹"""
    return jsonify({"success": False, "error": "æ­¤åŠŸèƒ½å·²ç¦ç”¨"}), 404

@app.route('/api/ml/compare_models', methods=['POST'])
def compare_models_endpoint():
    """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹æ€§èƒ½çš„APIç«¯ç‚¹"""
    return jsonify({"success": False, "error": "æ­¤åŠŸèƒ½å·²ç¦ç”¨"}), 404

@app.route('/api/ml/ensemble', methods=['POST'])
def build_ensemble_model_endpoint():
    """æ„å»ºé›†æˆæ¨¡å‹çš„APIç«¯ç‚¹"""
    return jsonify({"success": False, "error": "æ­¤åŠŸèƒ½å·²ç¦ç”¨"}), 404

@app.route('/api/ml/deploy', methods=['POST'])
def deploy_model_endpoint():
    """éƒ¨ç½²æ¨¡å‹çš„APIç«¯ç‚¹ (åç«¯ç”Ÿæˆç«¯ç‚¹)"""
    return jsonify({"success": False, "error": "æ­¤åŠŸèƒ½å·²ç¦ç”¨"}), 404

@app.route('/api/ml/deployments', methods=['GET'])
def get_deployed_models_endpoint():
    """è·å–å·²éƒ¨ç½²æ¨¡å‹åˆ—è¡¨çš„APIç«¯ç‚¹"""
    return jsonify({"success": False, "error": "æ­¤åŠŸèƒ½å·²ç¦ç”¨"}), 404

@app.route('/api/ml/undeploy/<deployment_id>', methods=['POST'])
def undeploy_model_endpoint(deployment_id):
    """å–æ¶ˆéƒ¨ç½²æ¨¡å‹çš„APIç«¯ç‚¹"""
    return jsonify({"success": False, "error": "æ­¤åŠŸèƒ½å·²ç¦ç”¨"}), 404

@app.route('/api/ml/explain', methods=['POST'])
def explain_model_endpoint():
    """è§£é‡Šæ¨¡å‹é¢„æµ‹çš„APIç«¯ç‚¹"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸ºç©º"}), 400

    required_fields = ['model_name', 'data_path', 'target_column']
    for field in required_fields:
        if field not in data:
            return jsonify({"error": f"ç¼ºå°‘å¿…è¦å­—æ®µ '{field}'"}), 400

    try:
        # å¯¼å…¥å¿…è¦çš„åº“
        import matplotlib.pyplot as plt
        import io
        import base64
        from sklearn.inspection import permutation_importance
        import pickle
        import os
        import shap

        model_name = data['model_name']
        data_path = data['data_path']
        target_column = data['target_column']

        # åŠ è½½æ¨¡å‹
        model_path = os.path.join("ml_models", f"{model_name}.pkl")
        if not os.path.exists(model_path):
            return jsonify({"error": f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨"}), 404

        with open(model_path, 'rb') as f:
            model = pickle.load(f)

        # åŠ è½½æ•°æ®
        try:
            if data_path.endswith('.csv'):
                df = pd.read_csv(data_path)
            elif data_path.endswith(('.xls', '.xlsx')):
                df = pd.read_excel(data_path)
            else:
                return jsonify({"error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒCSVå’ŒExcel"}), 400
        except Exception as e:
            app.logger.error(f"åŠ è½½æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return jsonify({"error": f"åŠ è½½æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"}), 500

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        if target_column not in df.columns:
            return jsonify({"error": f"ç›®æ ‡åˆ— {target_column} ä¸åœ¨æ•°æ®é›†ä¸­"}), 400

        # å¤„ç†ç¼ºå¤±å€¼
        df = df.dropna()

        X = df.drop(columns=[target_column])
        y = df[target_column]

        # è·å–æ¨¡å‹ç±»å‹
        model_type = type(model).__name__

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = {}
        feature_importance_plot = ""
        shap_plot = ""
        model_params = {}

        # è·å–æ¨¡å‹å‚æ•°
        try:
            model_params = model.get_params()
        except Exception as e:
            app.logger.warning(f"è·å–æ¨¡å‹å‚æ•°æ—¶å‡ºé”™: {e}")
            model_params = {"error": "æ— æ³•è·å–æ¨¡å‹å‚æ•°"}

        if hasattr(model, 'feature_importances_'):
            # å¯¹äºéšæœºæ£®æ—ã€å†³ç­–æ ‘ç­‰æœ‰feature_importances_å±æ€§çš„æ¨¡å‹
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]
            feature_names = X.columns

            for i, idx in enumerate(indices):
                feature_importance[feature_names[idx]] = float(importances[idx])

            # åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾
            plt.figure(figsize=(10, 6))
            plt.title("ç‰¹å¾é‡è¦æ€§")
            plt.bar(range(len(indices)), importances[indices], align='center')
            plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=90)
            plt.tight_layout()

            # å°†å›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
            buf = io.BytesIO()
            plt.savefig(buf, format='png')
            buf.seek(0)
            feature_importance_plot = base64.b64encode(buf.read()).decode('utf-8')
            plt.close()

            # å°è¯•ç”ŸæˆSHAPå€¼è§£é‡Š
            try:
                 # å¯¹äºæ ‘æ¨¡å‹ï¼Œä½¿ç”¨TreeExplainer
                 if hasattr(model, 'estimators_') or 'Tree' in model_type:
                     explainer = shap.TreeExplainer(model)
                     shap_values = explainer.shap_values(X.iloc[:100])  # ä½¿ç”¨å‰100ä¸ªæ ·æœ¬ä»¥æé«˜æ€§èƒ½

                     plt.figure(figsize=(12, 8))
                     if isinstance(shap_values, list):
                         # åˆ†ç±»æ¨¡å‹å¯èƒ½è¿”å›æ¯ä¸ªç±»åˆ«çš„SHAPå€¼åˆ—è¡¨
                         shap.summary_plot(shap_values[0], X.iloc[:100], show=False)
                     else:
                         # å›å½’æ¨¡å‹è¿”å›å•ä¸ªSHAPå€¼æ•°ç»„
                         shap.summary_plot(shap_values, X.iloc[:100], show=False)

                     buf = io.BytesIO()
                     plt.savefig(buf, format='png')
                     buf.seek(0)
                     shap_plot = base64.b64encode(buf.read()).decode('utf-8')
                     plt.close()
            except Exception as e:
                 app.logger.warning(f"ç”ŸæˆSHAPè§£é‡Šæ—¶å‡ºé”™: {e}")
                 # é”™è¯¯ä¸ä¼šä¸­æ–­æµç¨‹ï¼Œåªæ˜¯æ²¡æœ‰SHAPå›¾
            plt.tight_layout()

            # å°†å›¾è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
            buf = io.BytesIO()
            plt.savefig(buf, format='png')
            buf.seek(0)
            feature_importance_plot = base64.b64encode(buf.read()).decode('utf-8')
            plt.close()

        elif hasattr(model, 'coef_'):
            # å¯¹äºçº¿æ€§æ¨¡å‹
            coefficients = model.coef_
            if len(coefficients.shape) == 1:
                # å•ç›®æ ‡å›å½’æˆ–äºŒåˆ†ç±»
                feature_names = X.columns
                for i, name in enumerate(feature_names):
                    feature_importance[name] = float(abs(coefficients[i]))

                # åˆ›å»ºç³»æ•°å›¾
                plt.figure(figsize=(10, 6))
                plt.title("ç‰¹å¾ç³»æ•°")
                plt.bar(feature_names, coefficients)
                plt.xticks(rotation=90)
                plt.tight_layout()

                # å°†å›¾è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
                buf = io.BytesIO()
                plt.savefig(buf, format='png')
                buf.seek(0)
                feature_importance_plot = base64.b64encode(buf.read()).decode('utf-8')
                plt.close()
            else:
                # å¤šåˆ†ç±»
                feature_names = X.columns
                avg_importance = np.mean(np.abs(coefficients), axis=0)
                for i, name in enumerate(feature_names):
                    feature_importance[name] = float(avg_importance[i])

                # åˆ›å»ºå¹³å‡ç³»æ•°å›¾
                plt.figure(figsize=(10, 6))
                plt.title("å¹³å‡ç‰¹å¾ç³»æ•° (ç»å¯¹å€¼)")
                plt.bar(feature_names, avg_importance)
                plt.xticks(rotation=90)
                plt.tight_layout()

                # å°†å›¾è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
                buf = io.BytesIO()
                plt.savefig(buf, format='png')
                buf.seek(0)
                feature_importance_plot = base64.b64encode(buf.read()).decode('utf-8')
                plt.close()
        else:
            # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œä½¿ç”¨æ’åˆ—é‡è¦æ€§
            perm_importance = permutation_importance(model, X, y, n_repeats=10, random_state=42)
            feature_names = X.columns
            for i, name in enumerate(feature_names):
                feature_importance[name] = float(perm_importance.importances_mean[i])

            # åˆ›å»ºæ’åˆ—é‡è¦æ€§å›¾
            plt.figure(figsize=(10, 6))
            plt.title("æ’åˆ—ç‰¹å¾é‡è¦æ€§")
            sorted_idx = perm_importance.importances_mean.argsort()[::-1]
            plt.bar(range(X.shape[1]), perm_importance.importances_mean[sorted_idx])
            plt.xticks(range(X.shape[1]), [feature_names[i] for i in sorted_idx], rotation=90)
            plt.tight_layout()

            # å°†å›¾è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
            buf = io.BytesIO()
            plt.savefig(buf, format='png')
            buf.seek(0)
            feature_importance_plot = base64.b64encode(buf.read()).decode('utf-8')
            plt.close()

        # æ¨¡å‹å‚æ•°
        model_params = {}
        for param, value in model.get_params().items():
            # ç¡®ä¿å€¼æ˜¯JSONå¯åºåˆ—åŒ–çš„
            if isinstance(value, (int, float, str, bool, type(None))):
                model_params[param] = value
            else:
                model_params[param] = str(value)

        # å‡†å¤‡æ¨¡å‹è§£é‡Šç»“æœ
        result = {
            "model_name": model_name,
            "model_type": model_type,
            "feature_importance": feature_importance,
            "feature_importance_plot": feature_importance_plot,
            "model_params": model_params,
            "data_shape": {"rows": X.shape[0], "columns": X.shape[1]},
            "column_names": X.columns.tolist(),
            "message": f"æˆåŠŸè§£é‡Š{model_type}æ¨¡å‹"
        }

        # å¦‚æœæœ‰SHAPå›¾ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if shap_plot:
            result["shap_plot"] = shap_plot
            result["has_shap_explanation"] = True
        else:
            result["has_shap_explanation"] = False

        # æ·»åŠ æ¨¡å‹ç‰¹å®šçš„è§£é‡Šä¿¡æ¯
        if hasattr(model, 'classes_'):
            result["classes"] = model.classes_.tolist() if hasattr(model.classes_, 'tolist') else [str(c) for c in model.classes_]
            result["problem_type"] = "classification"
        else:
            result["problem_type"] = "regression"

        return jsonify(result), 200
    except Exception as e:
        app.logger.error(f"/api/ml/explain æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"error": f"è§£é‡Šæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}), 500

@app.route('/api/ml/auto_select', methods=['POST'])
def auto_model_selection_endpoint():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹çš„APIç«¯ç‚¹"""
    data = request.get_json()
    if not data:
        return jsonify({"error": "è¯·æ±‚ä½“ä¸ºç©º"}), 400

    required_fields = ['data_path', 'target_column']
    for field in required_fields:
        if field not in data:
            return jsonify({"error": f"ç¼ºå°‘å¿…è¦å­—æ®µ '{field}'"}), 400

    try:
        # ç›´æ¥è°ƒç”¨ml_models.pyä¸­çš„auto_model_selectionå‡½æ•°
        from ml_models import auto_model_selection

        data_path = data['data_path']
        target_column = data['target_column']
        categorical_columns = data.get('categorical_columns', [])
        numerical_columns = data.get('numerical_columns', [])
        cv = data.get('cv', 5)
        metric = data.get('metric', 'auto')
        models_to_try = data.get('models_to_try')

        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
        result = auto_model_selection(
            data_path=data_path,
            target_column=target_column,
            categorical_columns=categorical_columns,
            numerical_columns=numerical_columns,
            cv=cv,
            metric=metric,
            models_to_try=models_to_try
        )

        # æ ¼å¼åŒ–ç»“æœä»¥ä¾¿å‰ç«¯æ˜¾ç¤º
        formatted_result = {
            "model_name": result["model_name"],
            "model_type": result["model_type"],
            "params": result["params"],
            "cv_score": result["cv_score"],
            "is_classification": result["is_classification"],
            "all_models_results": result["all_models_results"],
            "message": f"æˆåŠŸé€‰æ‹©æœ€ä½³æ¨¡å‹: {result['model_type']}ï¼Œæ¨¡å‹åç§°ä¸º{result['model_name']}"
        }

        return jsonify(formatted_result), 200
    except Exception as e:
        app.logger.error(f"/api/ml/auto_select æ¥å£å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return jsonify({"error": f"è‡ªåŠ¨é€‰æ‹©æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}), 500

def check_config_and_kb():
    """æ£€æŸ¥åŸºæœ¬é…ç½®å’ŒçŸ¥è¯†åº“ç›®å½•ã€‚"""
    config_valid = True

    # æ£€æŸ¥APIå¯†é’¥é…ç½®
    if not AI_STUDIO_API_KEY:
        app.logger.error("é”™è¯¯ï¼šAI_STUDIO_API_KEY æœªåœ¨ .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­é…ç½®ã€‚")
        config_valid = False

    # æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•
    if not os.path.exists(KNOWLEDGE_BASE_DIR):
        app.logger.warning(f"è­¦å‘Šï¼šçŸ¥è¯†åº“ç›®å½• '{KNOWLEDGE_BASE_DIR}' ä¸å­˜åœ¨ã€‚")
        try:
            os.makedirs(KNOWLEDGE_BASE_DIR, exist_ok=True) # exist_ok=True é¿å…ç›®å½•å·²å­˜åœ¨æ—¶æŠ¥é”™
            app.logger.info(f"å·²è‡ªåŠ¨åˆ›å»ºçŸ¥è¯†åº“ç›®å½•: {KNOWLEDGE_BASE_DIR}ã€‚è¯·å°†æ‚¨çš„æ–‡æ¡£æ”¾å…¥æ­¤ç›®å½•ã€‚")
        except OSError as e:
            app.logger.error(f"æ— æ³•åˆ›å»ºçŸ¥è¯†åº“ç›®å½• {KNOWLEDGE_BASE_DIR}: {e}ã€‚è¯·æ‰‹åŠ¨åˆ›å»ºã€‚")
            config_valid = False
    else:
        # æ£€æŸ¥ç›®å½•æ˜¯å¦ä¸ºç©º
        current_files = os.listdir(KNOWLEDGE_BASE_DIR)
        if not current_files:
            app.logger.warning(f"è­¦å‘Šï¼šçŸ¥è¯†åº“ç›®å½• '{KNOWLEDGE_BASE_DIR}' ä¸ºç©ºã€‚RAGç³»ç»Ÿå°†æ²¡æœ‰å¯æŸ¥è¯¢çš„æ•°æ®æºã€‚")

    # æ£€æŸ¥ä¸Šä¼ ç›®å½•
    uploads_dir = os.path.join(os.getcwd(), 'uploads')
    if not os.path.exists(uploads_dir):
        app.logger.warning(f"è­¦å‘Šï¼šä¸Šä¼ ç›®å½• '{uploads_dir}' ä¸å­˜åœ¨ã€‚")
        try:
            os.makedirs(uploads_dir, exist_ok=True)
            app.logger.info(f"å·²è‡ªåŠ¨åˆ›å»ºä¸Šä¼ ç›®å½•: {uploads_dir}ã€‚")
        except OSError as e:
            app.logger.error(f"æ— æ³•åˆ›å»ºä¸Šä¼ ç›®å½• {uploads_dir}: {e}ã€‚è¯·æ‰‹åŠ¨åˆ›å»ºã€‚")
            config_valid = False

    # æ£€æŸ¥æ¨¡å‹ç›®å½•
    models_dir = os.path.join(os.getcwd(), 'ml_models')
    if not os.path.exists(models_dir):
        app.logger.warning(f"è­¦å‘Šï¼šæ¨¡å‹ç›®å½• '{models_dir}' ä¸å­˜åœ¨ã€‚")
        try:
            os.makedirs(models_dir, exist_ok=True)
            app.logger.info(f"å·²è‡ªåŠ¨åˆ›å»ºæ¨¡å‹ç›®å½•: {models_dir}ã€‚")
        except OSError as e:
            app.logger.error(f"æ— æ³•åˆ›å»ºæ¨¡å‹ç›®å½• {models_dir}: {e}ã€‚è¯·æ‰‹åŠ¨åˆ›å»ºã€‚")
            config_valid = False

    return config_valid

# é…ç½®é™æ€æ–‡ä»¶è·¯å¾„ï¼Œä½¿å‰ç«¯èƒ½å¤Ÿæ­£ç¡®åŠ è½½JavaScriptæ–‡ä»¶
# Functions is_rag_result_poor and get_direct_llm_answer have been moved to the top of the file.
@app.route('/templates/<path:filename>')
def serve_template_file(filename):
    """æä¾›æ¨¡æ¿ç›®å½•ä¸­çš„é™æ€æ–‡ä»¶"""
    try:
        template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')
        return send_from_directory(template_dir, filename)
    except Exception as e:
        app.logger.error(f"åŠ è½½æ¨¡æ¿æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        return f"æ— æ³•åŠ è½½æ–‡ä»¶ {filename}", 404

@app.route('/static/<path:filename>')
def serve_static_file(filename):
    """æä¾›é™æ€æ–‡ä»¶ç›®å½•ä¸­çš„æ–‡ä»¶"""
    try:
        static_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static')
        return send_from_directory(static_dir, filename)
    except Exception as e:
        app.logger.error(f"åŠ è½½é™æ€æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        return f"æ— æ³•åŠ è½½æ–‡ä»¶ {filename}", 404

# æ·»åŠ å­¦ä¹ è·¯å¾„ç›¸å…³APIç«¯ç‚¹
@app.route('/api/learning_path/create', methods=['POST'])
def create_learning_path():
    """åˆ›å»ºå­¦ä¹ è·¯å¾„API"""
    try:
        data = request.json
        required_fields = ['user_id', 'goal', 'prior_knowledge', 'weekly_hours']
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if not all(field in data for field in required_fields):
            return jsonify({
                'success': False,
                'error': f'Missing required fields. Required: {", ".join(required_fields)}'
            }), 400
            
        # ç”Ÿæˆå­¦ä¹ è·¯å¾„
        learning_path = generate_learning_path(
            user_id=data['user_id'],
            goal=data['goal'],
            prior_knowledge=data['prior_knowledge'],
            weekly_hours=data['weekly_hours'],
            max_modules=data.get('max_modules', 20)
        )
        
        return jsonify({
            'success': True,
            'message': 'å­¦ä¹ è·¯å¾„åˆ›å»ºæˆåŠŸ',
            'path': learning_path
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/learning_path/user/<user_id>', methods=['GET'])
def get_user_paths(user_id):
    """è·å–ç”¨æˆ·å­¦ä¹ è·¯å¾„API"""
    try:
        paths = get_user_learning_path(user_id)
        
        return jsonify({
            'success': True,
            'paths': paths
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/learning_path/update_progress', methods=['POST'])
def update_learning_progress():
    """æ›´æ–°å­¦ä¹ è·¯å¾„è¿›åº¦API"""
    try:
        data = request.json
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if 'path_id' not in data:
            return jsonify({
                'success': False,
                'error': 'Missing required field: path_id'
            }), 400
            
        # æ›´æ–°è¿›åº¦
        updated_path = update_path_progress(
            path_id=data['path_id'],
            completed_module_id=data.get('completed_module_id'),
            current_module_id=data.get('current_module_id')
        )
        
        return jsonify({
            'success': True,
            'message': 'å­¦ä¹ è¿›åº¦æ›´æ–°æˆåŠŸ',
            'path': updated_path
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/learning_path/predict/mastery', methods=['POST'])
def predict_mastery():
    """é¢„æµ‹æ¨¡å—æŒæ¡ç¨‹åº¦API"""
    try:
        data = request.json
        required_fields = ['user_id', 'module_id', 'weekly_hours']
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if not all(field in data for field in required_fields):
            return jsonify({
                'success': False,
                'error': f'Missing required fields. Required: {", ".join(required_fields)}'
            }), 400
            
        # é¢„æµ‹æŒæ¡ç¨‹åº¦
        prediction = predict_module_mastery(
            user_id=data['user_id'],
            module_id=data['module_id'],
            weekly_hours=data['weekly_hours'],
            focus_level=data.get('focus_level', 'medium')
        )
        
        return jsonify({
            'success': True,
            'prediction': prediction
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/learning_path/predict/completion_time', methods=['POST'])
def predict_path_completion():
    """é¢„æµ‹å­¦ä¹ è·¯å¾„å®Œæˆæ—¶é—´API"""
    try:
        data = request.json
        required_fields = ['user_id', 'path_id', 'weekly_hours']
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if not all(field in data for field in required_fields):
            return jsonify({
                'success': False,
                'error': f'Missing required fields. Required: {", ".join(required_fields)}'
            }), 400
            
        # é¢„æµ‹å®Œæˆæ—¶é—´
        prediction = predict_completion_time(
            user_id=data['user_id'],
            path_id=data['path_id'],
            weekly_hours=data['weekly_hours']
        )
        
        return jsonify({
            'success': True,
            'prediction': prediction
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# æŠ€æœ¯å®éªŒå®¤APIè·¯ç”±
@app.route('/api/tech_lab/models', methods=['GET'])
def get_models():
    """è·å–å¯ç”¨æ¨¡å‹API"""
    try:
        models = get_available_models()
        
        return jsonify({
            'success': True,
            'models': models
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/tech_lab/models/<model_id>', methods=['GET'])
def get_model(model_id):
    """è·å–æ¨¡å‹è¯¦æƒ…API"""
    try:
        model = get_model_details(model_id)
        
        return jsonify({
            'success': True,
            'model': model
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/tech_lab/experiments', methods=['GET'])
def get_experiments():
    """è·å–æ‰€æœ‰å®éªŒAPI"""
    try:
        experiments = get_all_experiments()
        
        return jsonify({
            'success': True,
            'experiments': experiments
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/tech_lab/experiments/<experiment_id>', methods=['GET'])
def get_experiment_details(experiment_id):
    """è·å–å®éªŒè¯¦æƒ…API"""
    try:
        experiment = get_experiment(experiment_id)
        
        return jsonify({
            'success': True,
            'experiment': experiment
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/tech_lab/experiments/create', methods=['POST'])
def create_new_experiment():
    """åˆ›å»ºå®éªŒAPI"""
    try:
        data = request.json
        required_fields = ['name', 'model_id', 'experiment_type', 'config']
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if not all(field in data for field in required_fields):
            return jsonify({
                'success': False,
                'error': f'Missing required fields. Required: {", ".join(required_fields)}'
            }), 400
            
        # åˆ›å»ºå®éªŒ
        experiment = create_experiment(
            name=data['name'],
            description=data.get('description', ''),
            model_id=data['model_id'],
            experiment_type=data['experiment_type'],
            config=data['config']
        )
        
        return jsonify({
            'success': True,
            'message': 'å®éªŒåˆ›å»ºæˆåŠŸ',
            'experiment': experiment
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/tech_lab/experiments/run', methods=['POST'])
def run_experiment_api():
    """è¿è¡Œå®éªŒAPI"""
    try:
        data = request.json
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        if 'experiment_id' not in data:
            return jsonify({
                'success': False,
                'error': 'Missing required field: experiment_id'
            }), 400
            
        # è¿è¡Œå®éªŒ
        experiment = run_experiment(
            experiment_id=data['experiment_id'],
            data_path=data.get('data_path')
        )
        
        return jsonify({
            'success': True,
            'message': 'å®éªŒè¿è¡ŒæˆåŠŸ',
            'experiment': experiment
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# åˆå§‹åŒ–åº”ç”¨ç¨‹åº
def init_app():
    """åˆå§‹åŒ–åº”ç”¨ç¨‹åºï¼ŒåŒ…æ‹¬é…ç½®æ£€æŸ¥å’ŒRAGç³»ç»Ÿåˆå§‹åŒ–"""
    if not check_config_and_kb():
        app.logger.critical("é…ç½®æ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·ä¿®å¤ä¸Šè¿°é—®é¢˜åé‡è¯•ã€‚ç¨‹åºå³å°†é€€å‡ºã€‚")
        return False

    app.logger.info("æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ (ç™¾åº¦æ–‡å¿ƒç‰ˆ)...")
    try:
        # é¦–æ¬¡è¿è¡Œæ—¶ï¼Œforce_recreate_vs=Falseã€‚å¦‚æœchroma_dbä¸å­˜åœ¨ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºã€‚
        # å¦‚æœå¸Œæœ›æ¯æ¬¡å¯åŠ¨éƒ½å¼ºåˆ¶é‡å»ºï¼ˆæ¯”å¦‚çŸ¥è¯†åº“æ–‡ä»¶ç»å¸¸å˜åŠ¨ï¼‰ï¼Œå¯ä»¥è®¾ä¸ºTrueï¼Œä½†ä¼šå¾ˆæ…¢ã€‚
        initialize_rag_system(force_recreate_vs=False)
        app.logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆã€‚")
        return True
    except Exception as e:
        app.logger.critical(f"RAGç³»ç»Ÿåˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}ã€‚", exc_info=True)
        return False

# ä¸»å‡½æ•°ï¼Œå¯åŠ¨Flaskåº”ç”¨
if __name__ == '__main__':
    try:
        # æ£€æŸ¥ä¾èµ–é¡¹
        try:
            import langchain_community
            import langchain
            import chromadb
        except ImportError as e:
            app.logger.critical(f"ç¼ºå°‘å¿…è¦çš„ä¾èµ–é¡¹: {e}ã€‚è¯·è¿è¡Œ 'pip install -r requirements.txt' å®‰è£…æ‰€æœ‰ä¾èµ–ã€‚")
            print(f"\né”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ–é¡¹: {e}")
            print("è¯·è¿è¡Œ 'pip install -r requirements.txt' å®‰è£…æ‰€æœ‰ä¾èµ–ã€‚\n")
            sys.exit(1)

        # åˆå§‹åŒ–åº”ç”¨
        if not init_app():
            app.logger.critical("åº”ç”¨åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºå³å°†é€€å‡ºã€‚")
            print("\né”™è¯¯: åº”ç”¨åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚\n")
            sys.exit(1)

        # ç¡®ä¿é™æ€æ–‡ä»¶ç›®å½•å­˜åœ¨
        static_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static')
        if not os.path.exists(static_dir):
            os.makedirs(static_dir, exist_ok=True)
            app.logger.info(f"å·²åˆ›å»ºé™æ€æ–‡ä»¶ç›®å½•: {static_dir}")

        # ç¡®ä¿templatesç›®å½•å­˜åœ¨
        templates_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')
        if not os.path.exists(templates_dir):
            app.logger.warning(f"æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {templates_dir}ï¼Œå°†åˆ›å»ºè¯¥ç›®å½•")
            os.makedirs(templates_dir, exist_ok=True)
            app.logger.info(f"å·²åˆ›å»ºæ¨¡æ¿ç›®å½•: {templates_dir}")

        # å¯åŠ¨æœåŠ¡å™¨
        app.logger.info(f"FlaskæœåŠ¡å™¨æ­£åœ¨å¯åŠ¨ï¼Œè¯·è®¿é—® http://localhost:5000 æˆ– http://127.0.0.1:5000")
        print(f"\næœåŠ¡å™¨å¯åŠ¨æˆåŠŸ! è¯·è®¿é—®: http://localhost:5000\n")
        # debug=True ç”¨äºå¼€å‘ï¼Œå®ƒä¼šè‡ªåŠ¨é‡è½½ä»£ç å¹¶æä¾›è°ƒè¯•å™¨ã€‚ç”Ÿäº§ç¯å¢ƒåº”è®¾ä¸º Falseã€‚
        # use_reloader=False å¯ä»¥é˜²æ­¢Flaskåœ¨debugæ¨¡å¼ä¸‹å¯åŠ¨ä¸¤æ¬¡ï¼ˆä¸€æ¬¡ä¸»è¿›ç¨‹ï¼Œä¸€æ¬¡é‡è½½è¿›ç¨‹ï¼‰ï¼Œ
        # è¿™å¯¹äºé¿å… initialize_rag_system è¢«æ‰§è¡Œä¸¤æ¬¡å¯èƒ½æœ‰ç”¨ã€‚
        app.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False)
    except Exception as e:
        app.logger.critical(f"å¯åŠ¨æœåŠ¡å™¨æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}", exc_info=True)
        print(f"\né”™è¯¯: å¯åŠ¨æœåŠ¡å™¨æ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}\n")
        sys.exit(1)
# ml_agents.py
import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import base64
from io import BytesIO
from typing import Dict, List, Any, Optional, Tuple
import colorsys
import uuid
import traceback

from langchain.tools import StructuredTool
from langchain_core.tools import Tool
from langchain_core.pydantic_v1 import Field
from pydantic import BaseModel
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain_core.prompts import PromptTemplate

from baidu_llm import BaiduErnieLLM
from ml_models import (
    train_model as actual_train_model, predict as actual_predict, list_available_models as actual_list_models,
    select_model_for_task as actual_select_model, load_model,
    create_ensemble_model, auto_model_selection as actual_auto_select,
    explain_model_prediction as actual_explain_prediction, compare_models as actual_compare_models,
    save_model_with_version, list_model_versions
)
from sklearn.metrics import confusion_matrix, classification_report, f1_score, mean_absolute_error, r2_score, \
    precision_score, accuracy_score, recall_score, mean_squared_error
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

# å®šä¹‰å·¥å…·è¾“å…¥æ¨¡å‹
class TrainModelInput(BaseModel):
    model_type: str = Field(..., description="æ¨¡å‹ç±»å‹ï¼Œå¦‚ 'linear_regression', 'logistic_regression', 'decision_tree', 'random_forest_classifier' ç­‰")
    data_path: str = Field(..., description="æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒCSVå’ŒExcelæ ¼å¼")
    target_column: str = Field(..., description="ç›®æ ‡åˆ—å")
    model_name: Optional[str] = Field(None, description="æ¨¡å‹ä¿å­˜åç§°ï¼Œå¦‚ä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ")
    categorical_columns: Optional[List[str]] = Field(None, description="åˆ†ç±»ç‰¹å¾åˆ—è¡¨")
    numerical_columns: Optional[List[str]] = Field(None, description="æ•°å€¼ç‰¹å¾åˆ—è¡¨")

class PredictInput(BaseModel):
    model_name: str = Field(..., description="æ¨¡å‹åç§°")
    input_data: Dict[str, Any] = Field(..., description="è¾“å…¥æ•°æ®ï¼Œæ ¼å¼ä¸ºå­—æ®µååˆ°å€¼çš„æ˜ å°„")

# ä¸ºå…¶ä»–å·¥å…·å®šä¹‰Pydanticæ¨¡å‹
class RecommendModelInput(BaseModel):
    task_description: str = Field(..., description="å¯¹ä»»åŠ¡çš„æè¿°ï¼Œä¾‹å¦‚'é¢„æµ‹æˆ¿ä»·'æˆ–'åˆ†ç±»åƒåœ¾é‚®ä»¶'")

class DataAnalysisInput(BaseModel):
    file_path: str = Field(..., description="æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒCSVå’ŒExcelæ ¼å¼")
    target_column: Optional[str] = Field(None, description="ç›®æ ‡åˆ—åï¼Œç”¨äºåˆ†æä¸ç›®æ ‡ç›¸å…³çš„ç‰¹å¾")
    analysis_type: Optional[str] = Field(None, description="åˆ†æç±»å‹ï¼Œä¾‹å¦‚ 'statistics', 'feature_relevance'")

class EvaluateModelInput(BaseModel):
    model_name: str = Field(..., description="æ¨¡å‹åç§°")
    test_data_path: str = Field(..., description="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„")
    target_column: str = Field(..., description="ç›®æ ‡åˆ—å")

class EnsembleModelInput(BaseModel):
    base_models: List[str] = Field(..., description="åŸºç¡€æ¨¡å‹åç§°åˆ—è¡¨")
    ensemble_type: str = Field("voting", description="é›†æˆç±»å‹ï¼Œå¯é€‰ 'voting', 'stacking', 'bagging'")
    weights: Optional[List[float]] = Field(None, description="åŸºç¡€æ¨¡å‹æƒé‡ï¼Œä»…ç”¨äºvotingé›†æˆ")
    save_name: Optional[str] = Field(None, description="ä¿å­˜çš„æ¨¡å‹åç§°")

class AutoSelectModelInput(BaseModel):
    data_path: str = Field(..., description="æ•°æ®æ–‡ä»¶è·¯å¾„")
    target_column: str = Field(..., description="ç›®æ ‡åˆ—å")
    categorical_columns: Optional[List[str]] = Field(None, description="åˆ†ç±»ç‰¹å¾åˆ—è¡¨")
    numerical_columns: Optional[List[str]] = Field(None, description="æ•°å€¼ç‰¹å¾åˆ—è¡¨")

class ExplainPredictionInput(BaseModel):
    model_name: str = Field(..., description="æ¨¡å‹åç§°")
    input_data: Dict[str, Any] = Field(..., description="è¾“å…¥æ•°æ®ï¼Œæ ¼å¼ä¸ºå­—æ®µååˆ°å€¼çš„æ˜ å°„")

class CompareModelsInput(BaseModel):
    model_names: List[str] = Field(..., description="æ¨¡å‹åç§°åˆ—è¡¨")
    test_data_path: str = Field(..., description="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„")
    target_column: str = Field(..., description="ç›®æ ‡åˆ—å")

class VersionModelInput(BaseModel):
    model_name: str = Field(..., description="æ¨¡å‹åç§°")
    version: Optional[str] = Field(None, description="ç‰ˆæœ¬å·ï¼Œå¦‚ä¸æä¾›åˆ™ä½¿ç”¨æ—¶é—´æˆ³")
    metadata: Optional[Dict[str, Any]] = Field(None, description="é¢å¤–çš„å…ƒæ•°æ®")

class ListModelVersionsInput(BaseModel):
    model_name: str = Field(..., description="æ¨¡å‹åç§°")

# ç”Ÿæˆæ¸å˜è‰²å½©åˆ—è¡¨
def generate_gradient_colors(n_colors):
    """ç”Ÿæˆæ¸å˜è‰²å½©åˆ—è¡¨ï¼Œç”¨äºå›¾è¡¨"""
    colors = []
    for i in range(n_colors):
        # ä»è“ç´«è‰²æ¸å˜åˆ°å¤©è“è‰²
        hue = 0.6 + (0.2 * i / max(1, n_colors - 1))  # è‰²ç›¸ä»0.6(è“ç´«)åˆ°0.8(å¤©è“)
        saturation = 0.7  # é¥±å’Œåº¦
        value = 0.9  # äº®åº¦
        rgb = colorsys.hsv_to_rgb(hue, saturation, value)
        # è½¬æ¢ä¸ºrgbaæ ¼å¼
        rgba = (rgb[0], rgb[1], rgb[2], 0.7)
        colors.append(rgba)
    return colors

# å¯è§†åŒ–å‡½æ•°
def generate_visualization(data_type, labels, values, title=None, options=None):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¹¶è¿”å›base64ç¼–ç """
    plt.figure(figsize=(10, 6))
    plt.style.use('ggplot')

    options = options or {}
    colors = generate_gradient_colors(len(values) if isinstance(values, list) else 5)

    if data_type == 'bar':
        plt.bar(labels, values, color=colors)
        plt.ylabel('å€¼')
    elif data_type == 'line':
        plt.plot(labels, values, marker='o', color='#4F46E5', linewidth=2)
        plt.ylabel('å€¼')
        plt.grid(True, alpha=0.3)
    elif data_type == 'pie':
        plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90,
                shadow=True, colors=colors)
        plt.axis('equal')  # ä½¿é¥¼å›¾ä¸ºæ­£åœ†å½¢
    elif data_type == 'scatter':
        plt.scatter(labels, values, color='#4F46E5', alpha=0.7, s=options.get('point_size', 70))
        plt.ylabel('å€¼')
    elif data_type == 'heatmap':
        # çƒ­åŠ›å›¾éœ€è¦äºŒç»´æ•°æ®
        if 'matrix' in options:
            sns.heatmap(options['matrix'], annot=True, fmt='.2f', cmap='Blues',
                       xticklabels=labels, yticklabels=options.get('y_labels', labels))
    elif data_type == 'radar':
        # é›·è¾¾å›¾(æåæ ‡æ¡å½¢å›¾)
        theta = np.linspace(0, 2*np.pi, len(labels), endpoint=False)
        values = np.array(values)
        ax = plt.subplot(111, polar=True)
        ax.fill(theta, values, color='#4F46E5', alpha=0.25)
        ax.plot(theta, values, color='#4F46E5', linewidth=2)
        ax.set_xticks(theta)
        ax.set_xticklabels(labels)
        ax.grid(True)
    elif data_type == 'bubble':
        # æ°”æ³¡å›¾ - éœ€è¦x, yåæ ‡å’Œå¤§å°æ•°æ®
        sizes = options.get('sizes', [50] * len(values))
        plt.scatter(labels, values, s=sizes, alpha=0.6, c=colors)
        plt.ylabel('å€¼')

    plt.title(title or 'æ•°æ®å¯è§†åŒ–')
    if data_type not in ['pie', 'radar', 'heatmap']:
        plt.xticks(rotation=45, ha='right')
    plt.tight_layout()

    # å°†å›¾åƒè½¬æ¢ä¸ºbase64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=100)
    buffer.seek(0)
    image_png = buffer.getvalue()
    buffer.close()
    plt.close()

    return {
        'type': data_type,
        'labels': labels,
        'values': values,
        'title': title,
        'options': options,
        'image': base64.b64encode(image_png).decode('utf-8')
    }

def visualize_feature_importance(model, feature_names, n_features=10):
    """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]

        top_n = min(n_features, len(feature_names))  # æœ€å¤šæ˜¾ç¤ºå‰nä¸ªç‰¹å¾
        plt.figure(figsize=(10, 6))
        plt.style.use('ggplot')
        plt.title('ç‰¹å¾é‡è¦æ€§')

        # ä½¿ç”¨æ¸å˜è‰²å½©
        colors = generate_gradient_colors(top_n)

        plt.bar(range(top_n), importances[indices][:top_n], color=colors, align='center')
        plt.xticks(range(top_n), [feature_names[i] for i in indices][:top_n], rotation=45, ha='right')
        plt.xlabel('ç‰¹å¾')
        plt.ylabel('é‡è¦æ€§')
        plt.tight_layout()

        # å°†å›¾åƒè½¬æ¢ä¸ºbase64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100)
        buffer.seek(0)
        image_png = buffer.getvalue()
        buffer.close()
        plt.close()

        # åŒæ—¶ç”Ÿæˆè¡¨æ ¼æ•°æ®
        table_data = {
            'columns': ['ç‰¹å¾', 'é‡è¦æ€§'],
            'data': [[feature_names[i], float(importances[indices][j])] for j, i in enumerate(indices[:top_n])]
        }

        return {
            'type': 'bar',
            'labels': [feature_names[i] for i in indices][:top_n],
            'values': importances[indices][:top_n].tolist(),
            'title': 'ç‰¹å¾é‡è¦æ€§',
            'image': base64.b64encode(image_png).decode('utf-8'),
            'table_data': table_data
        }
    return None

def visualize_confusion_matrix(y_true, y_pred, class_names=None):
    """å¯è§†åŒ–æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))

    # ç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()

    # å°†å›¾åƒè½¬æ¢ä¸ºbase64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=100)
    buffer.seek(0)
    image_png = buffer.getvalue()
    buffer.close()
    plt.close()

    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ç­‰æŒ‡æ ‡
    precision = np.diag(cm) / np.sum(cm, axis=0)
    recall = np.diag(cm) / np.sum(cm, axis=1)

    # å¤„ç†é™¤ä»¥é›¶çš„æƒ…å†µ
    precision = np.nan_to_num(precision)
    recall = np.nan_to_num(recall)

    # ç”Ÿæˆè¡¨æ ¼æ•°æ®
    metrics_data = []
    for i, class_name in enumerate(class_names):
        metrics_data.append([
            class_name,
            int(np.sum(cm[i, :])),  # è¯¥ç±»çš„æ€»æ ·æœ¬æ•°
            int(cm[i, i]),  # æ­£ç¡®é¢„æµ‹æ•°
            float(precision[i]),  # ç²¾ç¡®ç‡
            float(recall[i])  # å¬å›ç‡
        ])

    table_data = {
        'columns': ['ç±»åˆ«', 'æ ·æœ¬æ•°', 'æ­£ç¡®é¢„æµ‹', 'ç²¾ç¡®ç‡', 'å¬å›ç‡'],
        'data': metrics_data
    }

    return {
        'type': 'confusion_matrix',
        'matrix': cm.tolist(),
        'class_names': class_names,
        'image': base64.b64encode(image_png).decode('utf-8'),
        'table_data': table_data
    }

def visualize_metrics(metrics_dict):
    """å¯è§†åŒ–è¯„ä¼°æŒ‡æ ‡"""
    # è¿‡æ»¤æ‰éæ•°å€¼å‹æŒ‡æ ‡
    numeric_metrics = {}
    for k, v in metrics_dict.items():
        if isinstance(v, (int, float)) and not isinstance(v, bool):
            numeric_metrics[k] = v

    if not numeric_metrics:
        return None

    plt.figure(figsize=(10, 6))
    plt.style.use('ggplot')

    # ä½¿ç”¨æ¸å˜è‰²å½©
    colors = generate_gradient_colors(len(numeric_metrics))

    plt.bar(numeric_metrics.keys(), numeric_metrics.values(), color=colors)
    plt.title('æ¨¡å‹è¯„ä¼°æŒ‡æ ‡')
    plt.ylim(0, max(1.0, max(numeric_metrics.values()) * 1.1))  # æ ¹æ®æ•°å€¼åŠ¨æ€è°ƒæ•´Yè½´
    plt.xticks(rotation=45, ha='right')
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()

    # å°†å›¾åƒè½¬æ¢ä¸ºbase64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=100)
    buffer.seek(0)
    image_png = buffer.getvalue()
    buffer.close()
    plt.close()

    # ç”Ÿæˆè¡¨æ ¼æ•°æ®
    table_data = {
        'columns': ['æŒ‡æ ‡', 'å€¼'],
        'data': [[k, v] for k, v in numeric_metrics.items()]
    }

    return {
        'type': 'bar',
        'labels': list(numeric_metrics.keys()),
        'values': list(numeric_metrics.values()),
        'title': 'æ¨¡å‹è¯„ä¼°æŒ‡æ ‡',
        'image': base64.b64encode(image_png).decode('utf-8'),
        'table_data': table_data
    }

def visualize_clusters(X, labels, feature_names=None, method='pca'):
    """å¯è§†åŒ–èšç±»ç»“æœ"""
    # å¦‚æœç‰¹å¾ç»´åº¦å¤§äº2ï¼Œä½¿ç”¨é™ç»´
    if X.shape[1] > 2:
        if method == 'pca':
            reducer = PCA(n_components=2)
        else:  # ä½¿ç”¨t-SNE
            reducer = TSNE(n_components=2, random_state=42)

        X_2d = reducer.fit_transform(X)
    else:
        X_2d = X

    # è·å–å”¯ä¸€çš„èšç±»æ ‡ç­¾
    unique_labels = np.unique(labels)
    n_clusters = len(unique_labels)

    # ç”Ÿæˆé¢œè‰²
    colors = generate_gradient_colors(n_clusters)

    plt.figure(figsize=(10, 8))
    for i, label in enumerate(unique_labels):
        mask = labels == label
        plt.scatter(X_2d[mask, 0], X_2d[mask, 1],
                   c=[colors[i]],
                   label=f'èšç±» {label}',
                   alpha=0.7,
                   s=80,
                   edgecolors='w')

    plt.legend()
    plt.title('èšç±»å¯è§†åŒ–')
    if method == 'pca':
        plt.xlabel('ä¸»æˆåˆ† 1')
        plt.ylabel('ä¸»æˆåˆ† 2')
    else:
        plt.xlabel('t-SNE ç»´åº¦ 1')
        plt.ylabel('t-SNE ç»´åº¦ 2')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # å°†å›¾åƒè½¬æ¢ä¸ºbase64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=100)
    buffer.seek(0)
    image_png = buffer.getvalue()
    buffer.close()
    plt.close()

    # è®¡ç®—æ¯ä¸ªèšç±»çš„æ ·æœ¬æ•°é‡
    cluster_counts = np.bincount(labels.astype(int))

    # ç”Ÿæˆè¡¨æ ¼æ•°æ®
    table_data = {
        'columns': ['èšç±»', 'æ ·æœ¬æ•°é‡', 'æ¯”ä¾‹'],
        'data': [[f'èšç±» {i}', int(count), float(count)/len(labels)] for i, count in enumerate(cluster_counts)]
    }

    return {
        'type': 'scatter',
        'image': base64.b64encode(image_png).decode('utf-8'),
        'table_data': table_data,
        'method': method,
        'n_clusters': n_clusters
    }

def generate_data_table(data, columns=None, max_rows=100):
    """å°†æ•°æ®è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼"""
    if isinstance(data, pd.DataFrame):
        df = data.copy()
    elif isinstance(data, list) and all(isinstance(item, dict) for item in data):
        df = pd.DataFrame(data)
    elif isinstance(data, dict) and all(isinstance(data[k], (list, np.ndarray)) for k in data):
        df = pd.DataFrame(data)
    else:
        try:
            df = pd.DataFrame(data)
        except:
            return None

    # æˆªå–æ•°æ®
    if len(df) > max_rows:
        df = df.head(max_rows)

    # ä½¿ç”¨æŒ‡å®šåˆ—æˆ–æ‰€æœ‰åˆ—
    if columns:
        df = df[columns]

    # è½¬æ¢ä¸ºè¡¨æ ¼æ•°æ®
    return {
        'columns': df.columns.tolist(),
        'data': df.values.tolist()
    }

# æ·»åŠ ç‰¹å¾é‡è¦æ€§é›·è¾¾å›¾å¯è§†åŒ–
def visualize_feature_importance_radar(feature_importance, title='ç‰¹å¾é‡è¦æ€§é›·è¾¾å›¾'):
    """ç”Ÿæˆç‰¹å¾é‡è¦æ€§é›·è¾¾å›¾"""
    # å‡†å¤‡æ•°æ®
    features = list(feature_importance.keys())
    values = list(feature_importance.values())

    # ç¡®ä¿å€¼ä¸ºæ­£æ•°ï¼Œå¹¶ä¸”æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
    values = np.array(values)
    if np.any(values < 0):
        # å¯¹äºæœ‰è´Ÿå€¼çš„æƒ…å†µï¼Œä½¿ç”¨MinMaxScalerå°†èŒƒå›´ç¼©æ”¾åˆ°0-1
        values = (values - np.min(values)) / (np.max(values) - np.min(values))
    else:
        # åªéœ€è¦æ ‡å‡†åŒ–åˆ°0-1èŒƒå›´
        values = values / np.max(values) if np.max(values) > 0 else values

    # åˆ›å»ºé›·è¾¾å›¾
    plt.figure(figsize=(10, 8))

    # è®¡ç®—è§’åº¦å˜é‡
    angles = np.linspace(0, 2*np.pi, len(features), endpoint=False)

    # é—­åˆå›¾å½¢
    values = np.concatenate((values, [values[0]]))
    angles = np.concatenate((angles, [angles[0]]))
    features = features + [features[0]]

    # ç»˜åˆ¶é›·è¾¾å›¾
    ax = plt.subplot(111, polar=True)
    ax.fill(angles, values, color='#4F46E5', alpha=0.25)
    ax.plot(angles, values, 'o-', color='#4F46E5', linewidth=2)

    # è®¾ç½®åˆ»åº¦æ ‡ç­¾
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(features[:-1])

    # è®¾ç½®yè½´åˆ»åº¦
    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])

    # æ·»åŠ æ ‡é¢˜
    plt.title(title)

    # å°†å›¾åƒè½¬æ¢ä¸ºbase64
    buffer = BytesIO()
    plt.savefig(buffer, format='png', dpi=100)
    buffer.seek(0)
    image_png = buffer.getvalue()
    buffer.close()
    plt.close()

    return {
        'type': 'radar',
        'labels': features[:-1],  # ç§»é™¤é—­åˆç‚¹
        'values': values[:-1].tolist(),  # ç§»é™¤é—­åˆç‚¹
        'title': title,
        'image': base64.b64encode(image_png).decode('utf-8')
    }

# å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒç»“æœ
def visualize_model_comparison(comparison_results, metric='auto'):
    """
    å¯è§†åŒ–æ¨¡å‹æ¯”è¾ƒç»“æœ

    Args:
        comparison_results: æ¨¡å‹æ¯”è¾ƒç»“æœ
        metric: ä½¿ç”¨çš„æŒ‡æ ‡ï¼Œ'auto'è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©

    Returns:
        å¯è§†åŒ–æ•°æ®
    """
    models = comparison_results['models']

    # æŒ‰æ¨¡å‹ç±»å‹åˆ†ç»„
    classifiers = [model for model in models if model.get('is_classifier', False)]
    regressors = [model for model in models if not model.get('is_classifier', False) and 'error' not in model]

    visualizations = []
    tables = []

    # å¤„ç†åˆ†ç±»å™¨
    if classifiers:
        model_names = [model['model_name'] for model in classifiers]

        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºæ¡å½¢å›¾
        metrics = ['accuracy', 'precision', 'recall', 'f1']
        for metric_name in metrics:
            values = [model['metrics'].get(metric_name, 0) for model in classifiers]

            plt.figure(figsize=(10, 6))
            plt.style.use('ggplot')

            colors = generate_gradient_colors(len(values))
            plt.bar(model_names, values, color=colors)
            plt.title(f'åˆ†ç±»å™¨æ¯”è¾ƒ - {metric_name}')
            plt.ylim(0, 1.05)  # åˆ†ç±»æŒ‡æ ‡é€šå¸¸åœ¨0-1èŒƒå›´å†…
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()

            # å°†å›¾åƒè½¬æ¢ä¸ºbase64
            buffer = BytesIO()
            plt.savefig(buffer, format='png', dpi=100)
            buffer.seek(0)
            image_png = buffer.getvalue()
            buffer.close()
            plt.close()

            visualizations.append({
                'type': 'bar',
                'title': f'åˆ†ç±»å™¨æ¯”è¾ƒ - {metric_name}',
                'labels': model_names,
                'values': values,
                'image': base64.b64encode(image_png).decode('utf-8')
            })

        # ä¸ºåˆ†ç±»å™¨åˆ›å»ºç»¼åˆè¡¨æ ¼
        classifier_table = {
            'columns': ['æ¨¡å‹', 'å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°'],
            'data': [
                [
                    model['model_name'],
                    model['metrics'].get('accuracy', '-'),
                    model['metrics'].get('precision', '-'),
                    model['metrics'].get('recall', '-'),
                    model['metrics'].get('f1', '-')
                ]
                for model in classifiers
            ]
        }
        tables.append({'title': 'åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”', 'data': classifier_table})

    # å¤„ç†å›å½’å™¨
    if regressors:
        model_names = [model['model_name'] for model in regressors]

        # ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ›å»ºæ¡å½¢å›¾
        metrics = ['mse', 'rmse', 'mae', 'r2']
        for metric_name in metrics:
            values = [model['metrics'].get(metric_name, 0) for model in regressors]

            # å¯¹äºR2ï¼Œæˆ‘ä»¬å¸Œæœ›å€¼è¶Šé«˜è¶Šå¥½ï¼›å¯¹äºå…¶ä»–è¯¯å·®æŒ‡æ ‡ï¼Œå€¼è¶Šä½è¶Šå¥½
            if metric_name == 'r2':
                plt.figure(figsize=(10, 6))
                plt.style.use('ggplot')

                colors = generate_gradient_colors(len(values))
                plt.bar(model_names, values, color=colors)
                plt.title(f'å›å½’å™¨æ¯”è¾ƒ - {metric_name}')
                plt.ylim(min(0, min(values) - 0.1), max(1, max(values) + 0.1))
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()
            else:
                plt.figure(figsize=(10, 6))
                plt.style.use('ggplot')

                colors = generate_gradient_colors(len(values))
                plt.bar(model_names, values, color=colors)
                plt.title(f'å›å½’å™¨æ¯”è¾ƒ - {metric_name} (è¶Šä½è¶Šå¥½)')
                plt.ylim(0, max(values) * 1.2)
                plt.xticks(rotation=45, ha='right')
                plt.tight_layout()

            # å°†å›¾åƒè½¬æ¢ä¸ºbase64
            buffer = BytesIO()
            plt.savefig(buffer, format='png', dpi=100)
            buffer.seek(0)
            image_png = buffer.getvalue()
            buffer.close()
            plt.close()

            visualizations.append({
                'type': 'bar',
                'title': f'å›å½’å™¨æ¯”è¾ƒ - {metric_name}',
                'labels': model_names,
                'values': values,
                'image': base64.b64encode(image_png).decode('utf-8')
            })

        # ä¸ºå›å½’å™¨åˆ›å»ºç»¼åˆè¡¨æ ¼
        regressor_table = {
            'columns': ['æ¨¡å‹', 'MSE', 'RMSE', 'MAE', 'RÂ²'],
            'data': [
                [
                    model['model_name'],
                    model['metrics'].get('mse', '-'),
                    model['metrics'].get('rmse', '-'),
                    model['metrics'].get('mae', '-'),
                    model['metrics'].get('r2', '-')
                ]
                for model in regressors
            ]
        }
        tables.append({'title': 'å›å½’å™¨æ€§èƒ½å¯¹æ¯”', 'data': regressor_table})

    # åˆ›å»ºæœ€ä½³æ¨¡å‹ç»¼åˆè§†å›¾
    if comparison_results.get('best_classifier') or comparison_results.get('best_regressor'):
        best_model_info = []

        if comparison_results.get('best_classifier'):
            best_model_info.append({
                'type': 'æœ€ä½³åˆ†ç±»å™¨',
                'name': comparison_results['best_classifier'],
                'metric': 'F1åˆ†æ•°'
            })

        if comparison_results.get('best_regressor'):
            best_model_info.append({
                'type': 'æœ€ä½³å›å½’å™¨',
                'name': comparison_results['best_regressor'],
                'metric': 'RÂ²åˆ†æ•°'
            })

        best_model_table = {
            'columns': ['æ¨¡å‹ç±»å‹', 'æ¨¡å‹åç§°', 'è¯„ä¼°æŒ‡æ ‡'],
            'data': [[info['type'], info['name'], info['metric']] for info in best_model_info]
        }
        tables.append({'title': 'æœ€ä½³æ¨¡å‹', 'data': best_model_table})

    return {
        'visualizations': visualizations,
        'tables': tables
    }

# å¯è§†åŒ–æ¨¡å‹è§£é‡Šç»“æœ
def visualize_model_explanation(explanation_result):
    """
    å¯è§†åŒ–æ¨¡å‹è§£é‡Šç»“æœ

    Args:
        explanation_result: æ¨¡å‹è§£é‡Šç»“æœ

    Returns:
        å¯è§†åŒ–æ•°æ®
    """
    visualizations = []
    tables = []

    # ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
    feature_importance = explanation_result.get('feature_importance', {})
    if feature_importance:
        # è½¬æ¢ä¸ºæ’åºåçš„åˆ—è¡¨
        sorted_features = sorted(feature_importance.items(), key=lambda x: abs(x[1]), reverse=True)
        top_features = sorted_features[:10]  # æœ€å¤šå±•ç¤ºå‰10ä¸ªç‰¹å¾

        feature_names = [item[0] for item in top_features]
        importance_values = [item[1] for item in top_features]

        # åˆ›å»ºæ¡å½¢å›¾
        plt.figure(figsize=(10, 6))
        plt.style.use('ggplot')

        colors = generate_gradient_colors(len(top_features))
        bars = plt.bar(feature_names, importance_values, color=colors)

        # æ·»åŠ æ­£è´Ÿå€¼ä¸åŒé¢œè‰²
        for i, value in enumerate(importance_values):
            if value < 0:
                bars[i].set_color('tomato')

        plt.title('ç‰¹å¾é‡è¦æ€§')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()

        # å°†å›¾åƒè½¬æ¢ä¸ºbase64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100)
        buffer.seek(0)
        image_png = buffer.getvalue()
        buffer.close()
        plt.close()

        visualizations.append({
            'type': 'bar',
            'title': 'ç‰¹å¾é‡è¦æ€§',
            'labels': feature_names,
            'values': importance_values,
            'image': base64.b64encode(image_png).decode('utf-8')
        })

        # å¦‚æœæœ‰è¶³å¤Ÿå¤šçš„ç‰¹å¾ï¼Œåˆ›å»ºé›·è¾¾å›¾
        if len(feature_importance) >= 3:
            radar_viz = visualize_feature_importance_radar(
                {k: abs(v) for k, v in feature_importance.items()},
                title='ç‰¹å¾é‡è¦æ€§é›·è¾¾å›¾'
            )
            visualizations.append(radar_viz)

        # åˆ›å»ºç‰¹å¾é‡è¦æ€§è¡¨æ ¼
        feature_table = {
            'columns': ['ç‰¹å¾', 'é‡è¦æ€§'],
            'data': [[name, value] for name, value in sorted_features]
        }
        tables.append({'title': 'ç‰¹å¾é‡è¦æ€§', 'data': feature_table})

    # ç‰¹å¾è´¡çŒ®å¯è§†åŒ–
    feature_contributions = explanation_result.get('feature_contributions', [])
    if feature_contributions:
        # æœ€å¤šå±•ç¤ºå‰10ä¸ªç‰¹å¾è´¡çŒ®
        top_contributions = feature_contributions[:10]

        feature_names = [item['feature'] for item in top_contributions]
        contribution_values = [item['contribution'] for item in top_contributions]

        # åˆ›å»ºæ¡å½¢å›¾
        plt.figure(figsize=(10, 6))
        plt.style.use('ggplot')

        colors = generate_gradient_colors(len(top_contributions))
        bars = plt.bar(feature_names, contribution_values, color=colors)

        # æ·»åŠ æ­£è´Ÿå€¼ä¸åŒé¢œè‰²
        for i, value in enumerate(contribution_values):
            if value < 0:
                bars[i].set_color('tomato')

        plt.title('ç‰¹å¾è´¡çŒ®')
        plt.xticks(rotation=45, ha='right')
        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        plt.tight_layout()

        # å°†å›¾åƒè½¬æ¢ä¸ºbase64
        buffer = BytesIO()
        plt.savefig(buffer, format='png', dpi=100)
        buffer.seek(0)
        image_png = buffer.getvalue()
        buffer.close()
        plt.close()

        visualizations.append({
            'type': 'bar',
            'title': 'ç‰¹å¾è´¡çŒ®',
            'labels': feature_names,
            'values': contribution_values,
            'image': base64.b64encode(image_png).decode('utf-8')
        })

        # åˆ›å»ºç‰¹å¾è´¡çŒ®è¡¨æ ¼
        contribution_table = {
            'columns': ['ç‰¹å¾', 'å€¼', 'é‡è¦æ€§', 'è´¡çŒ®'],
            'data': [
                [
                    item['feature'],
                    item['value'],
                    item['importance'],
                    item['contribution']
                ]
                for item in feature_contributions
            ]
        }
        tables.append({'title': 'ç‰¹å¾è´¡çŒ®', 'data': contribution_table})

    # æ·»åŠ é¢„æµ‹ç»“æœè¡¨æ ¼
    prediction = explanation_result.get('prediction', [])
    if prediction:
        prediction_table = {
            'columns': ['é¢„æµ‹ç»“æœ'],
            'data': [[p] for p in prediction]
        }
        tables.append({'title': 'é¢„æµ‹ç»“æœ', 'data': prediction_table})

    return {
        'visualizations': visualizations,
        'tables': tables
    }

# åˆ›å»ºæœºå™¨å­¦ä¹ å·¥å…·
def create_ml_tools():
    """åˆ›å»ºæœºå™¨å­¦ä¹ å·¥å…·é›†"""

    def _train_model(
        model_type: str,
        data_path: str,
        target_column: str,
        model_name: str = None,
        categorical_columns: List[str] = None,
        numerical_columns: List[str] = None
    ) -> str:
        """è®­ç»ƒä¸€ä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹"""
        try:
            # Handle .xlsx input: convert to CSV before calling actual_train_model
            processed_data_path = data_path
            if data_path.lower().endswith( ('.xls', '.xlsx') ):
                try:
                    df = pd.read_excel(data_path)
                    # Create a temporary CSV file path in the uploads directory
                    # Generate a unique filename to avoid conflicts
                    temp_csv_filename = f"{os.path.splitext(os.path.basename(data_path))[0]}_temp_{uuid.uuid4().hex}.csv"
                    uploads_dir = os.path.join(os.getcwd(), 'uploads') # Assuming 'uploads' is in the current working directory
                    os.makedirs(uploads_dir, exist_ok=True)
                    processed_data_path = os.path.join(uploads_dir, temp_csv_filename)
                    df.to_csv(processed_data_path, index=False)
                    print(f"Converted Excel file {data_path} to temporary CSV: {processed_data_path}")
                except Exception as e:
                    return json.dumps({"text": f"âŒ è½¬æ¢Excelæ–‡ä»¶ä¸ºCSVæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", "error": f"è½¬æ¢Excelæ–‡ä»¶ä¸ºCSVæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"}) # Unicode for cross mark

            result = actual_train_model(
                model_type=model_type,
                data=processed_data_path, # Use the potentially converted path
                target_column=target_column,
                model_name=model_name,
                categorical_columns=categorical_columns,
                numerical_columns=numerical_columns
            )
            
            # Optionally, clean up the temporary CSV file after training
            if processed_data_path != data_path and os.path.exists(processed_data_path):
                 try:
                     os.remove(processed_data_path)
                     print(f"Cleaned up temporary CSV file: {processed_data_path}")
                 except Exception as e:
                     print(f"Warning: Failed to clean up temporary CSV file {processed_data_path}: {e}")

            # Format result for Agent output
            formatted_result = {
                "model_name": result.get("model_name", model_name or model_type),
                "model_type": result.get("model_type", model_type),
                "metrics": result.get("metrics", {}),
                "message": f"âœ… æˆåŠŸè®­ç»ƒäº†{result.get('model_type', model_type)}æ¨¡å‹ã€‚æ¨¡å‹åç§°ä¸º: {result.get('model_name', '')}ã€‚è¯„ä¼°æŒ‡æ ‡: {json.dumps(result.get('metrics', {}), ensure_ascii=False)}"
            }
            return json.dumps(formatted_result, ensure_ascii=False)
        except Exception as e:
            return json.dumps({"text": f"âŒ è®­ç»ƒæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", "error": str(e)}) # Unicode for cross mark

    def _predict_with_model(model_name: str, input_data: Dict[str, Any]) -> str:
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            result = actual_predict(model_name=model_name, input_data=input_data)

            # å°è¯•åŠ è½½æ¨¡å‹è·å–æ›´å¤šä¿¡æ¯
            model, _, metadata = load_model(model_name)
            model_info = {}

            # æ£€æŸ¥æ˜¯åˆ†ç±»è¿˜æ˜¯å›å½’
            is_classification = hasattr(model, "predict_proba")

            # æ ¼å¼åŒ–è¾“å‡º
            predictions = result["predictions"]
            pred_text = ""
            if len(predictions) == 1:  # å•ä¸ªé¢„æµ‹ç»“æœ
                pred_text = f"é¢„æµ‹ç»“æœ: {predictions[0]}"

                # å¦‚æœæ˜¯åˆ†ç±»æ¨¡å‹å¹¶ä¸”æœ‰æ¦‚ç‡è¾“å‡º
                if is_classification and "probabilities" in result:
                    probs = result["probabilities"][0]

                    # å¯è§†åŒ–æ¦‚ç‡åˆ†å¸ƒ
                    if isinstance(probs, dict):  # ç±»åˆ«åˆ°æ¦‚ç‡çš„æ˜ å°„
                        classes = list(probs.keys())
                        probabilities = list(probs.values())

                        vis_data = generate_visualization(
                            'bar',
                            classes,
                            probabilities,
                            title='é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ'
                        )

                        table_data = {
                            'columns': ['ç±»åˆ«', 'æ¦‚ç‡'],
                            'data': [[c, p] for c, p in zip(classes, probabilities)]
                        }

                        return json.dumps({
                            "text": pred_text,
                            "visualization_data": vis_data,
                            "table_data": table_data,
                            "predictions": predictions,
                            "probabilities": probs
                        })
            else:  # å¤šä¸ªé¢„æµ‹ç»“æœ
                pred_text = f"é¢„æµ‹ç»“æœ:\n" + "\n".join([f"- {p}" for p in predictions[:10]])
                if len(predictions) > 10:
                    pred_text += f"\n... ç­‰ {len(predictions)} æ¡ç»“æœ"

                # ç”Ÿæˆé¢„æµ‹ç»“æœè¡¨æ ¼
                table_data = {
                    'columns': ['ç´¢å¼•', 'é¢„æµ‹å€¼'],
                    'data': [[i, p] for i, p in enumerate(predictions[:100])]
                }

                # å¦‚æœæ˜¯åˆ†ç±»é¢„æµ‹ï¼Œå°è¯•å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
                if is_classification:
                    # ç»Ÿè®¡ç±»åˆ«é¢‘ç‡
                    from collections import Counter
                    counts = Counter(predictions)
                    categories = list(counts.keys())
                    frequencies = list(counts.values())

                    vis_data = generate_visualization(
                        'pie',
                        categories,
                        frequencies,
                        title='é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ'
                    )

                    return json.dumps({
                        "text": pred_text,
                        "visualization_data": vis_data,
                        "table_data": table_data,
                        "predictions": predictions
                    })

            return json.dumps({
                "text": pred_text,
                "predictions": predictions
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ é¢„æµ‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _list_models() -> str:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        try:
            models = actual_list_models()

            if not models:
                return json.dumps({
                    "text": "ğŸ“š æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å·²è®­ç»ƒçš„æ¨¡å‹ã€‚",
                    "models": []
                })

            # ç”Ÿæˆè¡¨æ ¼æ•°æ®
            table_data = {
                'columns': ['æ¨¡å‹åç§°', 'æ¨¡å‹ç±»å‹', 'ç‰¹å¾æ•°é‡', 'è®­ç»ƒæ—¶é—´'],
                'data': []
            }

            # æ”¶é›†æ¨¡å‹ç±»å‹è®¡æ•°ç”¨äºé¥¼å›¾
            model_types = {}

            model_list_text = "ğŸ“š å¯ç”¨æ¨¡å‹åˆ—è¡¨:\n\n"
            for model in models:
                model_list_text += f"- {model['name']} ({model['type']})\n"

                # æ·»åŠ åˆ°è¡¨æ ¼æ•°æ®
                table_data['data'].append([
                    model['name'],
                    model['type'],
                    model.get('n_features', 'æœªçŸ¥'),
                    model.get('created_at', 'æœªçŸ¥')
                ])

                # ç»Ÿè®¡æ¨¡å‹ç±»å‹
                model_type = model['type']
                if model_type in model_types:
                    model_types[model_type] += 1
                else:
                    model_types[model_type] = 1

            # åˆ›å»ºæ¨¡å‹ç±»å‹åˆ†å¸ƒå›¾
            types = list(model_types.keys())
            counts = list(model_types.values())

            if types:
                vis_data = generate_visualization(
                    'pie',
                    types,
                    counts,
                    title='æ¨¡å‹ç±»å‹åˆ†å¸ƒ'
                )
            else:
                vis_data = None

            return json.dumps({
                "text": model_list_text,
                "visualization_data": vis_data,
                "table_data": table_data,
                "models": models
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _recommend_model(task_description: str) -> str:
        """æ ¹æ®ä»»åŠ¡æè¿°æ¨èæ¨¡å‹"""
        try:
            result = actual_select_model(task_description)

            if not result:
                return json.dumps({
                    "text": "âŒ æ— æ³•ä¸ºæ­¤ä»»åŠ¡æ¨èåˆé€‚çš„æ¨¡å‹ã€‚è¯·æä¾›æ›´è¯¦ç»†çš„ä»»åŠ¡æè¿°ï¼Œæˆ–å°è¯•ä½¿ç”¨é€šç”¨æ¨¡å‹å¦‚éšæœºæ£®æ—ã€‚"
                })

            # æ¨èå¤šä¸ªæ¨¡å‹
            recommendations = result.get('recommendations', [])
            if recommendations:
                rec_text = f"ğŸ“Š æ ¹æ®ä»»åŠ¡ \"{task_description}\" çš„æ¨èæ¨¡å‹:\n\n"

                for idx, rec in enumerate(recommendations):
                    model_type = rec.get('model_type', 'æœªçŸ¥')
                    confidence = rec.get('confidence', 0) * 100
                    reason = rec.get('reason', 'æ— è¯´æ˜')

                    rec_text += f"{idx+1}. {model_type} (ç½®ä¿¡åº¦: {confidence:.1f}%)\n   åŸå› : {reason}\n\n"

                # ç”Ÿæˆå¯è§†åŒ–
                models = [r.get('model_type', 'å…¶ä»–') for r in recommendations]
                scores = [r.get('confidence', 0) * 100 for r in recommendations]

                vis_data = generate_visualization(
                    'bar',
                    models,
                    scores,
                    title='æ¨¡å‹æ¨èè¯„åˆ†'
                )

                # ç”Ÿæˆè¡¨æ ¼
                table_data = {
                    'columns': ['æ¨¡å‹ç±»å‹', 'ç½®ä¿¡åº¦', 'æ¨èç†ç”±'],
                    'data': [
                        [r.get('model_type', 'æœªçŸ¥'), f"{r.get('confidence', 0)*100:.1f}%", r.get('reason', 'æ— è¯´æ˜')]
                        for r in recommendations
                    ]
                }

                return json.dumps({
                    "text": rec_text,
                    "visualization_data": vis_data,
                    "table_data": table_data,
                    "recommendations": recommendations
                })
            else:
                rec_model = result.get('recommended_model', 'æœªçŸ¥')
                reason = result.get('reason', 'æ— è¯´æ˜')

                return json.dumps({
                    "text": f"ğŸ“Š å¯¹äºä»»åŠ¡ \"{task_description}\"ï¼Œæ¨èä½¿ç”¨ {rec_model} æ¨¡å‹ã€‚\n\nåŸå› : {reason}",
                    "recommended_model": rec_model,
                    "reason": reason
                })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ æ¨èæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _data_analysis(file_path: str, target_column: Optional[str] = None, analysis_type: Optional[str] = None) -> str:
        """åˆ†ææ•°æ®é›†å¹¶æä¾›ç»Ÿè®¡ä¿¡æ¯å’Œå¯è§†åŒ–"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                return json.dumps({
                    "text": f"âŒ æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ã€‚"
                })

            # æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ•°æ®
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            elif file_path.endswith(('.xls', '.xlsx')):
                df = pd.read_excel(file_path)
            else:
                return json.dumps({
                    "text": f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ã€‚ç›®å‰æ”¯æŒ CSV å’Œ Excel æ–‡ä»¶ã€‚"
                })

            # åŸºæœ¬æ•°æ®ä¿¡æ¯
            n_rows, n_cols = df.shape
            missing_values = df.isnull().sum().sum()
            missing_percent = missing_values / (n_rows * n_cols) * 100

            # æ•°æ®ç±»å‹åˆ†æ
            numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
            numeric_cols = df.select_dtypes(include=numerics).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
            datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()

            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            basic_stats = df.describe().transpose()

            # å‡†å¤‡å“åº”æ–‡æœ¬
            analysis_text = (f"ğŸ“Š æ•°æ®åˆ†ææŠ¥å‘Š - {file_path.split('/')[-1]}\n\n"
                           f"â–¶ åŸºæœ¬ä¿¡æ¯:\n"
                           f"  - è¡Œæ•°: {n_rows}\n"
                           f"  - åˆ—æ•°: {n_cols}\n"
                           f"  - ç¼ºå¤±å€¼: {missing_values} ({missing_percent:.2f}%)\n\n"
                           f"â–¶ åˆ—ç±»å‹åˆ†å¸ƒ:\n"
                           f"  - æ•°å€¼å‹: {len(numeric_cols)} åˆ—\n"
                           f"  - ç±»åˆ«å‹: {len(categorical_cols)} åˆ—\n"
                           f"  - æ—¶é—´å‹: {len(datetime_cols)} åˆ—\n")

            # å‡†å¤‡å¯è§†åŒ–
            visualizations = []
            tables = []

            # 1. åˆ—ç±»å‹åˆ†å¸ƒé¥¼å›¾
            type_vis = generate_visualization(
                'pie',
                ['æ•°å€¼å‹', 'ç±»åˆ«å‹', 'æ—¶é—´å‹'],
                [len(numeric_cols), len(categorical_cols), len(datetime_cols)],
                title='åˆ—ç±»å‹åˆ†å¸ƒ'
            )
            visualizations.append(type_vis)

            # 2. ä¸ºæ•°å€¼å‹åˆ—åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
            if numeric_cols:
                numeric_stats = df[numeric_cols].describe().transpose().reset_index()
                numeric_stats.columns = ['åˆ—å', 'è®¡æ•°', 'å‡å€¼', 'æ ‡å‡†å·®', 'æœ€å°å€¼', '25%', '50%', '75%', 'æœ€å¤§å€¼']

                # å¯¹é½å°æ•°ä½æ•°
                for col in numeric_stats.columns[2:]:
                    numeric_stats[col] = numeric_stats[col].apply(lambda x: f"{x:.4f}")

                numeric_table = {
                    'columns': numeric_stats.columns.tolist(),
                    'data': numeric_stats.values.tolist()
                }
                tables.append({
                    'title': 'æ•°å€¼å‹åˆ—ç»Ÿè®¡',
                    'data': numeric_table
                })

                # 3. ç”Ÿæˆä¸€äº›æ•°å€¼åˆ—çš„åˆ†å¸ƒå›¾ (é€‰æ‹©å‰5ä¸ª)
                for i, col in enumerate(numeric_cols[:5]):
                    if df[col].nunique() > 1:  # ç¡®ä¿ä¸æ˜¯å¸¸æ•°åˆ—
                        try:
                            plt.figure(figsize=(10, 6))
                            sns.histplot(df[col].dropna(), kde=True)
                            plt.title(f'{col} åˆ†å¸ƒ')
                            plt.tight_layout()

                            # å°†å›¾åƒè½¬æ¢ä¸ºbase64
                            buffer = BytesIO()
                            plt.savefig(buffer, format='png', dpi=100)
                            buffer.seek(0)
                            image_png = buffer.getvalue()
                            buffer.close()
                            plt.close()

                            visualizations.append({
                                'type': 'histogram',
                                'title': f'{col} åˆ†å¸ƒ',
                                'image': base64.b64encode(image_png).decode('utf-8')
                            })
                        except:
                            # ç•¥è¿‡æ— æ³•å¯è§†åŒ–çš„åˆ—
                            pass

            # 4. ç±»åˆ«å‹åˆ—çš„é¢‘ç‡åˆ†æ
            if categorical_cols:
                cat_tables = []
                for col in categorical_cols[:5]:  # åˆ†æå‰5ä¸ªç±»åˆ«åˆ—
                    if df[col].nunique() <= 20:  # é™åˆ¶å€¼çš„æ•°é‡
                        freq = df[col].value_counts().reset_index()
                        freq.columns = ['å€¼', 'é¢‘ç‡']
                        freq['ç™¾åˆ†æ¯”'] = freq['é¢‘ç‡'] / freq['é¢‘ç‡'].sum() * 100
                        freq['ç™¾åˆ†æ¯”'] = freq['ç™¾åˆ†æ¯”'].apply(lambda x: f"{x:.2f}%")

                        cat_table = {
                            'columns': freq.columns.tolist(),
                            'data': freq.values.tolist()
                        }

                        cat_tables.append({
                            'title': f'{col} é¢‘ç‡åˆ†å¸ƒ',
                            'data': cat_table
                        })

                        # ç”Ÿæˆç±»åˆ«é¢‘ç‡å›¾
                        try:
                            top_n = min(10, df[col].nunique())
                            top_cats = df[col].value_counts().nlargest(top_n)

                            plt.figure(figsize=(10, 6))
                            sns.barplot(x=top_cats.index, y=top_cats.values)
                            plt.title(f'{col} å‰{top_n}ç±»åˆ«é¢‘ç‡')
                            plt.xticks(rotation=45, ha='right')
                            plt.tight_layout()

                            # å°†å›¾åƒè½¬æ¢ä¸ºbase64
                            buffer = BytesIO()
                            plt.savefig(buffer, format='png', dpi=100)
                            buffer.seek(0)
                            image_png = buffer.getvalue()
                            buffer.close()
                            plt.close()

                            visualizations.append({
                                'type': 'bar',
                                'title': f'{col} å‰{top_n}ç±»åˆ«é¢‘ç‡',
                                'image': base64.b64encode(image_png).decode('utf-8')
                            })
                        except:
                            # ç•¥è¿‡æ— æ³•å¯è§†åŒ–çš„åˆ—
                            pass

                tables.extend(cat_tables)

            # 5. ç›¸å…³æ€§åˆ†æ
            if analysis_type == 'feature_relevance' and target_column and target_column in df.columns and numeric_cols:
                analysis_text += f"\nğŸ¯ ä¸ç›®æ ‡å˜é‡ '{target_column}' ç›¸å…³çš„ç‰¹å¾åˆ†æ:\n"
                # ç¡®ä¿ç›®æ ‡åˆ—æ˜¯æ•°å€¼ç±»å‹æ‰èƒ½è®¡ç®—ç›¸å…³æ€§
                if target_column not in numeric_cols:
                    analysis_text += f"è­¦å‘Š: ç›®æ ‡åˆ— '{target_column}' ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œæ— æ³•è®¡ç®—å…¶ä¸å…¶ä»–æ•°å€¼ç‰¹å¾çš„ç›¸å…³ç³»æ•°ã€‚\n"
                else:
                    try:
                        # è®¡ç®—æ‰€æœ‰æ•°å€¼åˆ—ä¸ç›®æ ‡åˆ—çš„ç›¸å…³æ€§
                        target_corr = df[numeric_cols].corr()[target_column].sort_values(ascending=False)
                        analysis_text += "ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§:\n"
                        # è¿‡æ»¤æ‰ç›®æ ‡åˆ—è‡ªèº«çš„ç›¸å…³æ€§ï¼ˆå€¼ä¸º1ï¼‰
                        filtered_target_corr = target_corr[target_corr.index != target_column]
                        analysis_text += filtered_target_corr.to_string() + "\n"

                        # å¯è§†åŒ–ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§ (æ¡å½¢å›¾)
                        plt.figure(figsize=(10, max(6, len(filtered_target_corr) * 0.3)))
                        # ä½¿ç”¨generate_gradient_colorsç”Ÿæˆé¢œè‰²åˆ—è¡¨
                        colors_for_target_corr = generate_gradient_colors(len(filtered_target_corr))
                        filtered_target_corr.plot(kind='barh', color=colors_for_target_corr)
                        plt.title(f'ç‰¹å¾ä¸ç›®æ ‡å˜é‡ "{target_column}" çš„ç›¸å…³æ€§')
                        plt.xlabel('ç›¸å…³ç³»æ•°')
                        plt.tight_layout()
                        
                        buffer = BytesIO()
                        plt.savefig(buffer, format='png', dpi=100)
                        buffer.seek(0)
                        image_png = buffer.getvalue()
                        buffer.close()
                        plt.close()

                        visualizations.append({
                            'type': 'bar',
                            'title': f'ç‰¹å¾ä¸ç›®æ ‡å˜é‡ "{target_column}" çš„ç›¸å…³æ€§',
                            'labels': filtered_target_corr.index.tolist(),
                            'values': filtered_target_corr.values.tolist(),
                            'image': base64.b64encode(image_png).decode('utf-8')
                        })

                        tables.append({
                            'title': f'ä¸ç›®æ ‡ "{target_column}" çš„ç›¸å…³æ€§',
                            'data': {
                                'columns': ['ç‰¹å¾', 'ç›¸å…³ç³»æ•°'],
                                'data': [[idx, round(val, 4)] for idx, val in filtered_target_corr.items()]
                            }
                        })
                    except Exception as e_corr_target:
                        analysis_text += f"è®¡ç®—ä¸ç›®æ ‡å˜é‡ '{target_column}' ç›¸å…³æ€§æ—¶å‡ºé”™: {str(e_corr_target)}\n"
                        # pass # ä¿æŒé”™è¯¯ä¿¡æ¯å¯è§
            
            elif len(numeric_cols) >= 2: # æ‰§è¡Œä¸€èˆ¬çš„ç›¸å…³æ€§åˆ†æ (å½“ analysis_type ä¸æ˜¯ feature_relevance æˆ– target_column æœªæä¾›æ—¶)
                try:
                    corr = df[numeric_cols].corr()

                    # ç›¸å…³æ€§çƒ­å›¾
                    plt.figure(figsize=(12, 10))
                    mask = np.triu(np.ones_like(corr, dtype=bool))
                    sns.heatmap(corr, mask=mask, cmap='coolwarm', annot=True, fmt='.2f',
                               linewidths=.5, cbar_kws={'shrink': .8})
                    plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ (æ‰€æœ‰æ•°å€¼ç‰¹å¾)')
                    plt.tight_layout()

                    # å°†å›¾åƒè½¬æ¢ä¸ºbase64
                    buffer = BytesIO()
                    plt.savefig(buffer, format='png', dpi=100)
                    buffer.seek(0)
                    image_png = buffer.getvalue()
                    buffer.close()
                    plt.close()

                    visualizations.append({
                        'type': 'heatmap',
                        'title': 'ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ',
                        'image': base64.b64encode(image_png).decode('utf-8')
                    })

                    # æå–å¼ºç›¸å…³ç‰¹å¾å¯¹
                    strong_corr = []
                    for i in range(len(corr.columns)):
                        for j in range(i+1, len(corr.columns)):
                            if abs(corr.iloc[i, j]) > 0.5:  # å¼ºç›¸å…³é˜ˆå€¼
                                strong_corr.append([
                                    corr.columns[i],
                                    corr.columns[j],
                                    corr.iloc[i, j]
                                ])

                    # æå–å¼ºç›¸å…³ç‰¹å¾å¯¹ (ç»å¯¹å€¼å¤§äº0.7)
                    strong_corr_pairs = []
                    for i in range(len(corr.columns)):
                        for j in range(i + 1, len(corr.columns)):
                            if abs(corr.iloc[i, j]) > 0.7:
                                strong_corr_pairs.append([
                                    corr.columns[i],
                                    corr.columns[j],
                                    round(corr.iloc[i, j], 4)
                                ])
                    # æŒ‰ç›¸å…³æ€§ç»å¯¹å€¼é™åºæ’åºï¼Œå–å‰15ä¸ª
                    strong_corr_pairs_sorted = sorted(strong_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:15]

                    if strong_corr_pairs_sorted:
                        corr_table_data = {
                            'columns': ['ç‰¹å¾1', 'ç‰¹å¾2', 'ç›¸å…³ç³»æ•°'],
                            'data': strong_corr_pairs_sorted
                        }
                        tables.append({
                            'title': 'å¼ºç›¸å…³ç‰¹å¾å¯¹ (Top 15, |ç›¸å…³ç³»æ•°| > 0.7)',
                            'data': corr_table_data
                        })
                except Exception as e_corr_general:
                    analysis_text += f"è®¡ç®—é€šç”¨ç›¸å…³æ€§çŸ©é˜µæ—¶å‡ºé”™: {str(e_corr_general)}\n"
                    # pass

            # 6. æ•°æ®é¢„è§ˆè¡¨æ ¼
            preview_table = {
                'columns': df.columns.tolist(),
                'data': df.head(10).values.tolist()
            }
            tables.append({
                'title': 'æ•°æ®é¢„è§ˆ',
                'data': preview_table
            })

            # è¿”å›JSONå“åº”
            return json.dumps({
                "text": analysis_text,
                "visualizations": visualizations,
                "tables": tables,
                "df_info": {
                    "shape": df.shape,
                    "columns": df.columns.tolist(),
                    "numeric_columns": numeric_cols,
                    "categorical_columns": categorical_cols,
                    "datetime_columns": datetime_cols
                }
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ åˆ†ææ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _evaluate_model(model_name: str, test_data_path: str, target_column: str) -> str:
        """
        è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„è¡¨ç°
        
        å‚æ•°:
            model_name: æ¨¡å‹åç§°
            test_data_path: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            target_column: ç›®æ ‡åˆ—å
            
        è¿”å›:
            JSONå­—ç¬¦ä¸²åŒ…å«è¯„ä¼°ç»“æœæˆ–é”™è¯¯ä¿¡æ¯
        """
        try:
            # è®°å½•è¯„ä¼°å¼€å§‹
            print(f"[INFO] å¼€å§‹è¯„ä¼°æ¨¡å‹ {model_name}...")
            
            # æ£€æŸ¥æ¨¡å‹å’Œæ•°æ®æ˜¯å¦å­˜åœ¨
            model_path = os.path.join('ml_models', f"{model_name}.pkl")
            if not os.path.exists(model_path):
                error_msg = f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨ã€‚"
                print(f"[ERROR] {error_msg}")
                return json.dumps({
                    "text": error_msg,
                    "error": "MODEL_NOT_FOUND"
                })

            if not os.path.exists(test_data_path):
                error_msg = f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ {test_data_path} ä¸å­˜åœ¨ã€‚"
                print(f"[ERROR] {error_msg}")
                return json.dumps({
                    "text": error_msg,
                    "error": "TEST_DATA_NOT_FOUND"
                })

            # åŠ è½½æ¨¡å‹
            model, preprocessors, _ = load_model(model_name)

            # åŠ è½½æµ‹è¯•æ•°æ®
            if test_data_path.endswith('.csv'):
                test_data = pd.read_csv(test_data_path)
            elif test_data_path.endswith(('.xls', '.xlsx')):
                test_data = pd.read_excel(test_data_path)
            else:
                error_msg = "âŒ ä¸æ”¯æŒçš„æµ‹è¯•æ•°æ®æ ¼å¼ã€‚ç›®å‰æ”¯æŒ CSV å’Œ Excel æ–‡ä»¶ã€‚"
                print(f"[ERROR] {error_msg}")
                return json.dumps({
                    "text": error_msg,
                    "error": "UNSUPPORTED_FILE_FORMAT"
                })

            # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
            if target_column not in test_data.columns:
                error_msg = f"âŒ ç›®æ ‡åˆ— {target_column} åœ¨æµ‹è¯•æ•°æ®ä¸­ä¸å­˜åœ¨ã€‚"
                print(f"[ERROR] {error_msg}")
                return json.dumps({
                    "text": error_msg,
                    "error": "TARGET_COLUMN_NOT_FOUND"
                })

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            X_test = test_data.drop(columns=[target_column])
            y_test = test_data[target_column]

            # åº”ç”¨é¢„å¤„ç†
            if 'label_encoders' in preprocessors:
                for col, encoder in preprocessors['label_encoders'].items():
                    if col in X_test.columns:
                        X_test[col] = X_test[col].astype(str)
                        try:
                            X_test[col] = encoder.transform(X_test[col])
                        except:
                            # å¤„ç†æœªçŸ¥ç±»åˆ«
                            X_test[col] = X_test[col].map(lambda x: 0 if x not in encoder.classes_ else encoder.transform([x])[0])

            if 'scaler' in preprocessors and hasattr(preprocessors['scaler'], 'transform'):
                if hasattr(preprocessors['scaler'], 'feature_names_in_'):
                    # Scikit-learn 1.0+
                    common_cols = [col for col in preprocessors['scaler'].feature_names_in_ if col in X_test.columns]
                    if common_cols:
                        X_test[common_cols] = preprocessors['scaler'].transform(X_test[common_cols])
            else:
                    # å°è¯•è¯†åˆ«æ•°å€¼åˆ—å¹¶ç¼©æ”¾
                    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
                    numeric_cols = X_test.select_dtypes(include=numerics).columns.tolist()
                    if numeric_cols:
                        X_test[numeric_cols] = preprocessors['scaler'].transform(X_test[numeric_cols])

            # è¿›è¡Œé¢„æµ‹
            y_pred = model.predict(X_test)

            # æ£€æŸ¥æ¨¡å‹ç±»å‹
            is_classifier = hasattr(model, "predict_proba")
            is_regressor = not is_classifier

            # æ ¹æ®æ¨¡å‹ç±»å‹è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            metrics = {}
            if is_classifier:
                # åˆ†ç±»æŒ‡æ ‡
                metrics["accuracy"] = accuracy_score(y_test, y_pred)
                metrics["precision"] = precision_score(y_test, y_pred, average="weighted", zero_division=0)
                metrics["recall"] = recall_score(y_test, y_pred, average="weighted", zero_division=0)
                metrics["f1"] = f1_score(y_test, y_pred, average="weighted", zero_division=0)

                # è·å–ç±»åˆ«åç§°
                if 'label_encoders' in preprocessors and target_column in preprocessors['label_encoders']:
                    class_names = preprocessors['label_encoders'][target_column].classes_
                else:
                    class_names = sorted(set(y_test.unique()))

                # è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
                report = classification_report(y_test, y_pred, output_dict=True)

                # æ··æ·†çŸ©é˜µå¯è§†åŒ–
                cm_vis = visualize_confusion_matrix(y_test, y_pred, class_names=class_names)

                # ä¸ºåˆ†ç±»æŠ¥å‘Šåˆ›å»ºè¡¨æ ¼
                report_data = []
                for class_name, metrics_dict in report.items():
                    if class_name not in ['accuracy', 'macro avg', 'weighted avg']:
                        report_data.append([
                            class_name,
                            metrics_dict['precision'],
                            metrics_dict['recall'],
                            metrics_dict['f1-score'],
                            metrics_dict['support']
                        ])

                # æ·»åŠ å¹³å‡æŒ‡æ ‡
                for avg_type in ['macro avg', 'weighted avg']:
                    if avg_type in report:
                        report_data.append([
                            avg_type,
                            report[avg_type]['precision'],
                            report[avg_type]['recall'],
                            report[avg_type]['f1-score'],
                            report[avg_type]['support']
                        ])

                report_table = {
                    'columns': ['ç±»åˆ«', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'æ ·æœ¬æ•°'],
                    'data': report_data
                }

                # åˆ›å»ºæŒ‡æ ‡å¯è§†åŒ–
                metrics_vis = visualize_metrics(metrics)

                # æ ¼å¼åŒ–è¯„ä¼°ç»“æœæ–‡æœ¬
                eval_text = (f"ğŸ“Š æ¨¡å‹ {model_name} è¯„ä¼°ç»“æœ:\n\n"
                           f"â–¶ æ¨¡å‹ç±»å‹: åˆ†ç±»å™¨\n"
                           f"â–¶ æµ‹è¯•æ ·æœ¬æ•°: {len(y_test)}\n"
                           f"â–¶ ä¸»è¦æŒ‡æ ‡:\n"
                           f"  - å‡†ç¡®ç‡: {metrics['accuracy']:.4f}\n"
                           f"  - ç²¾ç¡®ç‡: {metrics['precision']:.4f}\n"
                           f"  - å¬å›ç‡: {metrics['recall']:.4f}\n"
                           f"  - F1åˆ†æ•°: {metrics['f1']:.4f}\n")

                return json.dumps({
                    "text": eval_text,
                    "visualizations": [cm_vis, metrics_vis],
                    "tables": [
                        {'title': 'åˆ†ç±»æŠ¥å‘Š', 'data': report_table},
                        {'title': 'æ··æ·†çŸ©é˜µ', 'data': cm_vis.get('table_data')}
                    ],
                    "metrics": metrics,
                    "model_type": "classifier"
                })

            else:
                # å›å½’æŒ‡æ ‡
                metrics["mse"] = mean_squared_error(y_test, y_pred)
                metrics["rmse"] = np.sqrt(metrics["mse"])
                metrics["mae"] = mean_absolute_error(y_test, y_pred)
                metrics["r2"] = r2_score(y_test, y_pred)

                # åˆ›å»ºé¢„æµ‹vså®é™…å€¼æ•£ç‚¹å›¾
                plt.figure(figsize=(10, 6))
                plt.scatter(y_test, y_pred, alpha=0.5)
                plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
                plt.xlabel('å®é™…å€¼')
                plt.ylabel('é¢„æµ‹å€¼')
                plt.title('é¢„æµ‹å€¼ vs å®é™…å€¼')
                plt.tight_layout()

                # å°†å›¾åƒè½¬æ¢ä¸ºbase64
                buffer = BytesIO()
                plt.savefig(buffer, format='png', dpi=100)
                buffer.seek(0)
                image_png = buffer.getvalue()
                buffer.close()
                plt.close()

                scatter_vis = {
                    'type': 'scatter',
                    'title': 'é¢„æµ‹å€¼ vs å®é™…å€¼',
                    'image': base64.b64encode(image_png).decode('utf-8')
                }

                # åˆ›å»ºæ®‹å·®å›¾
                residuals = y_test - y_pred
                plt.figure(figsize=(10, 6))
                plt.scatter(y_pred, residuals, alpha=0.5)
                plt.axhline(y=0, color='r', linestyle='--')
                plt.xlabel('é¢„æµ‹å€¼')
                plt.ylabel('æ®‹å·®')
                plt.title('æ®‹å·®å›¾')
                plt.tight_layout()

                # å°†å›¾åƒè½¬æ¢ä¸ºbase64
                buffer = BytesIO()
                plt.savefig(buffer, format='png', dpi=100)
                buffer.seek(0)
                image_png = buffer.getvalue()
                buffer.close()
                plt.close()

                residual_vis = {
                    'type': 'scatter',
                    'title': 'æ®‹å·®å›¾',
                    'image': base64.b64encode(image_png).decode('utf-8')
                }

                # åˆ›å»ºæŒ‡æ ‡å¯è§†åŒ–
                metrics_vis = visualize_metrics(metrics)

                # ä¸ºæ®‹å·®åˆ›å»ºè¡¨æ ¼ï¼ˆå‰20ä¸ªæ ·æœ¬ï¼‰
                residual_data = pd.DataFrame({
                    'å®é™…å€¼': y_test.iloc[:20],
                    'é¢„æµ‹å€¼': y_pred[:20],
                    'æ®‹å·®': residuals.iloc[:20]
                }).reset_index().rename(columns={'index': 'æ ·æœ¬ç´¢å¼•'})

                residual_table = {
                    'columns': residual_data.columns.tolist(),
                    'data': residual_data.values.tolist()
                }

                # æ ¼å¼åŒ–è¯„ä¼°ç»“æœæ–‡æœ¬
                eval_text = (f"ğŸ“Š æ¨¡å‹ {model_name} è¯„ä¼°ç»“æœ:\n\n"
                           f"â–¶ æ¨¡å‹ç±»å‹: å›å½’å™¨\n"
                           f"â–¶ æµ‹è¯•æ ·æœ¬æ•°: {len(y_test)}\n"
                           f"â–¶ ä¸»è¦æŒ‡æ ‡:\n"
                           f"  - MSE (å‡æ–¹è¯¯å·®): {metrics['mse']:.4f}\n"
                           f"  - RMSE (å‡æ–¹æ ¹è¯¯å·®): {metrics['rmse']:.4f}\n"
                           f"  - MAE (å¹³å‡ç»å¯¹è¯¯å·®): {metrics['mae']:.4f}\n"
                           f"  - RÂ² (å†³å®šç³»æ•°): {metrics['r2']:.4f}\n")

                return json.dumps({
                    "text": eval_text,
                    "visualizations": [scatter_vis, residual_vis, metrics_vis],
                    "tables": [
                        {'title': 'æ®‹å·®åˆ†æ', 'data': residual_table}
                    ],
                    "metrics": metrics,
                    "model_type": "regressor"
                })

        except Exception as e:
            import traceback
            error_msg = f"âŒ è¯„ä¼°æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            print(f"[ERROR] {error_msg}")
            print(f"[DEBUG] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return json.dumps({
                "text": error_msg,
                "error": str(e),
                "error_type": type(e).__name__,
                "traceback": traceback.format_exc()
            })

    # æ·»åŠ æ–°çš„å·¥å…·å‡½æ•°
    def _create_ensemble_model(
        base_models: List[str],
        ensemble_type: str = 'voting',
        weights: List[float] = None,
        save_name: str = None
    ) -> str:
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        try:
            # å°† voting_classifier æ˜ å°„åˆ° voting
            if ensemble_type == 'voting_classifier':
                ensemble_type = 'voting'
            
            result = create_ensemble_model(
                base_models=base_models,
                ensemble_type=ensemble_type,
                weights=weights,
                save_name=save_name
            )

            # æ ¼å¼åŒ–è¾“å‡º
            model_info = f"âœ… é›†æˆæ¨¡å‹åˆ›å»ºæˆåŠŸ!\n\n"
            model_info += f"ğŸ“Š æ¨¡å‹åç§°: {result['model_name']}\n"
            model_info += f"ğŸ“ˆ é›†æˆç±»å‹: {ensemble_type}\n"
            model_info += f"ğŸ“‘ åŸºç¡€æ¨¡å‹: {', '.join(base_models)}\n"

            if weights:
                model_info += f"âš–ï¸ æƒé‡: {weights}\n"

            return json.dumps({
                "text": model_info,
                "result": {
                    "model_name": result['model_name'],
                    "ensemble_type": ensemble_type,
                    "base_models": base_models
                }
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ åˆ›å»ºé›†æˆæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _auto_select_model(
        data_path: str,
        target_column: str,
        categorical_columns: List[str] = None,
        numerical_columns: List[str] = None
    ) -> str:
        """è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹å¹¶ä¼˜åŒ–è¶…å‚æ•°"""
        try:
            result = actual_auto_select(
                data_path=data_path,
                target_column=target_column,
                categorical_columns=categorical_columns,
                numerical_columns=numerical_columns
            )

            # è·å–æ¨¡å‹ç±»å‹å’Œå‚æ•°
            model_type = result['model_type']
            params = result['params']
            cv_score = result['cv_score']

            # æ ¼å¼åŒ–å‚æ•°è¾“å‡º
            params_str = ", ".join([f"{k}={v}" for k, v in params.items()])

            # æ ¼å¼åŒ–è¾“å‡º
            model_info = f"âœ… è‡ªåŠ¨æ¨¡å‹é€‰æ‹©å®Œæˆ!\n\n"
            model_info += f"ğŸ† æœ€ä½³æ¨¡å‹: {model_type}\n"
            model_info += f"ğŸ“Š æ¨¡å‹åç§°: {result['model_name']}\n"
            model_info += f"âš™ï¸ æœ€ä½³å‚æ•°: {params_str}\n"
            model_info += f"ğŸ“ˆ äº¤å‰éªŒè¯åˆ†æ•°: {cv_score:.4f}\n\n"

            # æ·»åŠ æ‰€æœ‰æ¨¡å‹çš„æ¯”è¾ƒç»“æœ
            model_info += "ğŸ“Š æ‰€æœ‰æ¨¡å‹æ¯”è¾ƒ:\n"
            for idx, model_result in enumerate(result['all_models_results']):
                model_info += f"{idx+1}. {model_result['model_type']}: CV={model_result['cv_score']:.4f}, Test={model_result['test_score']:.4f}\n"

            # å‡†å¤‡å¯è§†åŒ–æ•°æ® - æ¨¡å‹æ¯”è¾ƒå›¾
            model_types = [m['model_type'] for m in result['all_models_results']]
            cv_scores = [m['cv_score'] for m in result['all_models_results']]

            vis_data = generate_visualization(
                'bar',
                model_types,
                cv_scores,
                title='æ¨¡å‹CVåˆ†æ•°æ¯”è¾ƒ'
            )

            # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
            table_data = {
                'columns': ['æ¨¡å‹ç±»å‹', 'CVåˆ†æ•°', 'æµ‹è¯•åˆ†æ•°', 'å‚æ•°'],
                'data': [
                    [
                        m['model_type'],
                        f"{m['cv_score']:.4f}",
                        f"{m['test_score']:.4f}",
                        str({k: v for k, v in m['best_params'].items()})
                    ]
                    for m in result['all_models_results']
                ]
            }

            return json.dumps({
                "text": model_info,
                "visualization_data": vis_data,
                "table_data": table_data,
                "result": result
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ è‡ªåŠ¨é€‰æ‹©æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _explain_prediction(
        model_name: str,
        input_data: Dict[str, Any]
    ) -> str:
        """è§£é‡Šæ¨¡å‹é¢„æµ‹ç»“æœ"""
        try:
            explanation = actual_explain_prediction(model_name, input_data)

            # è·å–é¢„æµ‹ç»“æœ
            prediction = explanation['prediction']
            prediction_str = str(prediction[0]) if len(prediction) == 1 else str(prediction)

            # æ ¼å¼åŒ–è¾“å‡º
            explanation_text = f"âœ… æ¨¡å‹é¢„æµ‹è§£é‡Šå®Œæˆ!\n\n"
            explanation_text += f"ğŸ“Š é¢„æµ‹ç»“æœ: {prediction_str}\n\n"

            # æ·»åŠ ç‰¹å¾é‡è¦æ€§ä¿¡æ¯
            if explanation['feature_importance']:
                explanation_text += "ğŸ“‘ ç‰¹å¾é‡è¦æ€§ (å‰5é¡¹):\n"
                sorted_features = sorted(
                    explanation['feature_importance'].items(),
                    key=lambda x: abs(x[1]),
                    reverse=True
                )[:5]

                for feature, importance in sorted_features:
                    explanation_text += f"  - {feature}: {importance:.4f}\n"

            # è·å–å¯è§†åŒ–ç»“æœ
            viz_results = visualize_model_explanation(explanation)

            return json.dumps({
                "text": explanation_text,
                "visualizations": viz_results['visualizations'],
                "tables": viz_results['tables'],
                "explanation": explanation
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ è§£é‡Šé¢„æµ‹ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _compare_models(
        model_names: List[str],
        test_data_path: str,
        target_column: str
    ) -> str:
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        try:
            if not model_names:
                return json.dumps({
                    "text": "âŒ æ¨¡å‹æ¯”è¾ƒå¤±è´¥ï¼šå¿…é¡»è‡³å°‘æä¾›ä¸€ä¸ªæ¨¡å‹åç§°ã€‚",
                    "error": "æ¨¡å‹åç§°åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚",
                    "comparison": None
                })
            # actual_compare_models is an alias for compare_models from ml_models
            comparison = actual_compare_models(model_names, test_data_path, target_column)

            # æ ¼å¼åŒ–è¾“å‡º
            comparison_text = f"âœ… æ¨¡å‹æ¯”è¾ƒå®Œæˆ!\n\n"
            comparison_text += f"ğŸ“Š æµ‹è¯•æ•°æ®: {test_data_path}\n"
            comparison_text += f"ğŸ¯ ç›®æ ‡åˆ—: {target_column}\n\n"

            # æ·»åŠ æœ€ä½³æ¨¡å‹ä¿¡æ¯
            if comparison.get('best_classifier'):
                comparison_text += f"ğŸ† æœ€ä½³åˆ†ç±»å™¨: {comparison['best_classifier']}\n"

            if comparison.get('best_regressor'):
                comparison_text += f"ğŸ† æœ€ä½³å›å½’å™¨: {comparison['best_regressor']}\n"

            # è·å–å¯è§†åŒ–ç»“æœ
            viz_results = visualize_model_comparison(comparison)

            return json.dumps({
                "text": comparison_text,
                "visualizations": viz_results['visualizations'],
                "tables": viz_results['tables'],
                "comparison": comparison
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ æ¯”è¾ƒæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _version_model(
        model_name: str,
        version: str = None,
        metadata: Dict[str, Any] = None
    ) -> str:
        """åˆ›å»ºæ¨¡å‹çš„ç‰ˆæœ¬"""
        try:
            # å…ˆåŠ è½½æ¨¡å‹
            loaded_data = load_model(model_name) # load_model is imported from ml_models
            if loaded_data is None or loaded_data[0] is None:
                error_msg = f"æ¨¡å‹ '{model_name}' æœªæ‰¾åˆ°æˆ–åŠ è½½å¤±è´¥ã€‚"
                if loaded_data is None:
                    error_msg = f"æ¨¡å‹æ–‡ä»¶ '{model_name}' æœªæ‰¾åˆ°æˆ–æ— æ³•åŠ è½½ã€‚"
                elif loaded_data[0] is None:
                    error_msg = f"æ¨¡å‹ '{model_name}' åŠ è½½æˆåŠŸï¼Œä½†æ¨¡å‹å¯¹è±¡ä¸ºç©ºã€‚"
                return json.dumps({
                    "text": f"âŒ åˆ›å»ºæ¨¡å‹ç‰ˆæœ¬æ—¶å‘ç”Ÿé”™è¯¯: {error_msg}",
                    "error": error_msg,
                    "version_info": None
                })
            model, preprocessors, _ = loaded_data

            # ä¿å­˜ç‰ˆæœ¬
            # save_model_with_version is imported from ml_models
            version_info = save_model_with_version(model, model_name, preprocessors, metadata, version)

            # æ ¼å¼åŒ–è¾“å‡º
            version_text = f"âœ… æ¨¡å‹ç‰ˆæœ¬ä¿å­˜æˆåŠŸ!\n\n"
            version_text += f"ğŸ“Š æ¨¡å‹åç§°: {version_info['model_name']}\n"
            version_text += f"ğŸ”– ç‰ˆæœ¬å·: {version_info['version']}\n"
            version_text += f"â±ï¸ æ—¶é—´æˆ³: {version_info['timestamp']}\n"

            # åˆ›å»ºè¡¨æ ¼æ•°æ®
            table_data = {
                'columns': ['å­—æ®µ', 'å€¼'],
                'data': [
                    [k, str(v)] for k, v in version_info.items()
                    if k not in ('path', 'timestamp')  # æ’é™¤ä¸€äº›ä¸éœ€è¦æ˜¾ç¤ºçš„å­—æ®µ
                ]
            }

            return json.dumps({
                "text": version_text,
                "table_data": table_data,
                "version_info": version_info
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ åˆ›å»ºæ¨¡å‹ç‰ˆæœ¬æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    def _list_model_versions(model_name: str) -> str:
        """åˆ—å‡ºæ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬"""
        try:
            versions = list_model_versions(model_name)

            if not versions:
                return json.dumps({
                    "text": f"ğŸ“Š æ¨¡å‹ {model_name} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç‰ˆæœ¬ã€‚"
                })

            # æ ¼å¼åŒ–è¾“å‡º
            version_text = f"ğŸ“Š æ¨¡å‹ {model_name} çš„ç‰ˆæœ¬åˆ—è¡¨:\n\n"

            for idx, version in enumerate(versions):
                v_num = version.get('version', 'unknown')
                v_time = version.get('timestamp', 'unknown')
                version_text += f"{idx+1}. ç‰ˆæœ¬ {v_num} (æ—¶é—´: {v_time})\n"

            # åˆ›å»ºè¡¨æ ¼æ•°æ®
            table_data = {
                'columns': ['ç‰ˆæœ¬å·', 'åˆ›å»ºæ—¶é—´', 'è·¯å¾„'],
                'data': [
                    [
                        v.get('version', '-'),
                        v.get('timestamp', '-'),
                        v.get('path', '-')
                    ]
                    for v in versions
                ]
            }

            return json.dumps({
                "text": version_text,
                "table_data": table_data,
                "versions": versions
            })
        except Exception as e:
            return json.dumps({
                "text": f"âŒ è·å–æ¨¡å‹ç‰ˆæœ¬åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "error": str(e)
            })

    # åˆ›å»ºå·¥å…·é›†
    tools = [
        StructuredTool.from_function(
            func=_train_model,
            name="train_model",
            description="è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œæ”¯æŒå¤šç§ç±»å‹çš„æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ç­‰",
            args_schema=TrainModelInput
        ),
        StructuredTool.from_function(
            func=_predict_with_model,
            name="predict_with_model",
            description="ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹",
            args_schema=PredictInput
        ),
        Tool.from_function(
            func=_list_models,
            name="list_models",
            description="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æœºå™¨å­¦ä¹ æ¨¡å‹"
        ),
        Tool.from_function(
            func=_recommend_model,
            name="recommend_model",
            description="æ ¹æ®ä»»åŠ¡æè¿°æ¨èé€‚åˆçš„æœºå™¨å­¦ä¹ æ¨¡å‹",
            args_schema=RecommendModelInput
        ),
        Tool.from_function(
            func=_data_analysis,
            name="analyze_data",
            description="åˆ†ææ•°æ®é›†çš„ç‰¹å¾ã€ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶èƒ½æ‰¾å‡ºä¸æŒ‡å®šç›®æ ‡å˜é‡æœ€ç›¸å…³çš„ç‰¹å¾ã€‚",
            args_schema=DataAnalysisInput
        ),
        Tool.from_function(
            func=_evaluate_model,
            name="evaluate_model",
            description="è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„è¡¨ç°",
            args_schema=EvaluateModelInput
        ),
        Tool.from_function(
            func=_create_ensemble_model,
            name="create_ensemble_model",
            description="åˆ›å»ºé›†æˆæ¨¡å‹ï¼Œç»„åˆå¤šä¸ªåŸºç¡€æ¨¡å‹",
            args_schema=EnsembleModelInput
        ),
        Tool.from_function(
            func=_auto_select_model,
            name="auto_select_model",
            description="è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹å¹¶ä¼˜åŒ–è¶…å‚æ•°",
            args_schema=AutoSelectModelInput
        ),
        Tool.from_function(
            func=_explain_prediction,
            name="explain_prediction",
            description="è§£é‡Šæ¨¡å‹é¢„æµ‹ç»“æœï¼Œæä¾›ç‰¹å¾é‡è¦æ€§å’Œè´¡çŒ®åˆ†æ",
            args_schema=ExplainPredictionInput
        ),
        Tool.from_function(
            func=_compare_models,
            name="compare_models",
            description="æ¯”è¾ƒå¤šä¸ªæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½",
            args_schema=CompareModelsInput
        ),
        Tool.from_function(
            func=_version_model,
            name="version_model",
            description="ä¸ºæ¨¡å‹åˆ›å»ºä¸€ä¸ªæ–°ç‰ˆæœ¬",
            args_schema=VersionModelInput
        ),
        Tool.from_function(
            func=_list_model_versions,
            name="list_model_versions",
            description="åˆ—å‡ºæ¨¡å‹çš„æ‰€æœ‰ç‰ˆæœ¬",
            args_schema=ListModelVersionsInput
        )
    ]

    return tools

def create_ml_agent(use_existing_model: bool = True):
    """åˆ›å»ºæœºå™¨å­¦ä¹ ä»£ç†"""
    model = BaiduErnieLLM()
    tools = create_ml_tools()

    # æå–å·¥å…·åç§°åˆ—è¡¨
    # æç¤ºæ¨¡æ¿ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å˜é‡
    # æ³¨æ„ï¼š`tools`å˜é‡å°†ç”±create_structured_chat_agentç”¨å·¥å…·çš„æ ¼å¼åŒ–æè¿°å¡«å……
    prompt_template_base = """
{tools}
Tool Names: {tool_names}
Input: {input}
{agent_scratchpad}
"""
    model_preference_text = ""
    if use_existing_model:
        model_preference_text = "\né‡è¦æç¤ºï¼šå½“å‰è®¾ç½®ä¸ºä¼˜å…ˆä½¿ç”¨å·²è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚é™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚æˆ–æ²¡æœ‰åˆé€‚çš„ç°æœ‰æ¨¡å‹ï¼Œå¦åˆ™è¯·ä¸è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚\n"

    # model_preference_text is already defined above.
    # prompt_template_base is already defined above and contains the necessary placeholders:
    # """
    # {tools}
    # Tool Names: {tool_names}
    # Input: {input}
    # {agent_scratchpad}
    # """

    # Combine model preference text with the base template
    effective_template_str = model_preference_text + prompt_template_base

    prompt = PromptTemplate(
        template=effective_template_str,
        input_variables=["input", "agent_scratchpad", "tools", "tool_names"]
        # Using default validate_template=True is recommended
    )

    # åˆ›å»ºä»£ç†
    agent = create_structured_chat_agent(
        llm=model,
        tools=tools,
        prompt=prompt
    )

    # åˆ›å»ºä»£ç†æ‰§è¡Œå™¨
    agent_executor = AgentExecutor.from_agent_and_tools(
        agent=agent,
        tools=tools,
        verbose=True,
        handle_parsing_errors=lambda e: f"JSONè§£æé”™è¯¯: {str(e)}\nåŸå§‹å“åº”:\n```json\n{e.response.replace('{', '{{').replace('}', '}}')}\n```\n",
        max_iterations=1,  # å‡å°‘æœ€å¤§è¿­ä»£æ¬¡æ•°ä»¥é¿å…è¶…æ—¶
        return_intermediate_steps=True,
        max_execution_time=180  # è®¾ç½®æœ€å¤§æ‰§è¡Œæ—¶é—´ä¸º30ç§’
    )

    return agent_executor

def query_ml_agent(question: str, use_existing_model: bool = True) -> Dict[str, Any]:
    """
    æŸ¥è¯¢æœºå™¨å­¦ä¹ ä»£ç†

    Args:
        question: ç”¨æˆ·é—®é¢˜

    Returns:
        åŒ…å«å›ç­”å’Œå¯èƒ½çš„å¯è§†åŒ–æ•°æ®çš„å­—å…¸
    """
    agent_response_output = ""
    visualization_data = None
    table_data = None
    is_ml_query_flag = True # é»˜è®¤ä¸º True
    error_message = None
    error_details = None
    expected_format_info = None


    try:
        # åˆ›å»ºå¹¶æŸ¥è¯¢ä»£ç†
        agent = create_ml_agent(use_existing_model=use_existing_model)
        response = agent.invoke({"input": question})

        # è§£æJSONå“åº”
        agent_response_output = response.get("output", "") # ä½¿ç”¨å±€éƒ¨å˜é‡
        steps = response.get("intermediate_steps", [])

        # æå–å¯èƒ½åŒ…å«çš„å¯è§†åŒ–æ•°æ®
        # visualization_data = None # å·²åœ¨å‡½æ•°å¼€å¤´åˆå§‹åŒ–
        # table_data = None # å·²åœ¨å‡½æ•°å¼€å¤´åˆå§‹åŒ–

        # å†…éƒ¨ try-except ç”¨äºå¤„ç†å·¥å…·è¾“å‡ºçš„è§£æ
        try:
            if steps and len(steps) > 0:
                last_step = steps[-1]
                tool_output = last_step[1]

                if isinstance(tool_output, str) and tool_output.strip().startswith('{'):
                    try:
                        json_output = json.loads(tool_output)
                        if 'text' in json_output:
                            agent_response_output = json_output['text']
                        if 'visualization_data' in json_output:
                            visualization_data = json_output['visualization_data']
                        if 'table_data' in json_output:
                            table_data = json_output['table_data']
                        if 'visualizations' in json_output:
                            if json_output['visualizations']:
                                visualization_data = json_output['visualizations'][0]
                        if 'tables' in json_output:
                            if json_output['tables']:
                                table_data = json_output['tables'][0]['data']
                    except (json.JSONDecodeError, ValueError) as e_json:
                        # è¿™ä¸ª return ä¼šæå‰ç»“æŸå‡½æ•°ï¼Œå¦‚æœè¿™æ˜¯æœŸæœ›è¡Œä¸ºåˆ™ä¿ç•™
                        # å¦åˆ™ï¼Œåº”è¯¥è®¾ç½®é”™è¯¯ä¿¡æ¯å¹¶ç»§ç»­åˆ°å‡½æ•°æœ«å°¾çš„ return
                        error_message = "æ¨¡å‹å“åº”æ ¼å¼é”™è¯¯"
                        error_details = f"è¯·ä¸¥æ ¼ä½¿ç”¨è¦æ±‚çš„JSONæ ¼å¼ã€‚é”™è¯¯ä¿¡æ¯: {str(e_json)}"
                        expected_format_info = {
                                "action": "tool_name",
                                "action_input": {"param1": "value"}
                            }
                        # å¦‚æœå¸Œæœ›åœ¨è¿™é‡Œå°±è¿”å›ï¼Œé‚£ä¹ˆï¼š
                        return {
                            "answer": agent_response_output, # æˆ–è€…ä¸€ä¸ªå›ºå®šçš„é”™è¯¯æç¤º
                            "visualization_data": visualization_data,
                            "table_data": table_data,
                            "is_ml_query": is_ml_query_flag,
                            "error": error_message,
                            "details": error_details,
                            "expected_format": expected_format_info
                        }
                # else: # ä¸æ˜¯JSONæ ¼å¼ï¼Œä¿æŒåŸæ ·
                #    pass # è¿™ä¸ª pass æ˜¯ä¸å¿…è¦çš„
        except Exception as e_inner_parse:
            # æå–å¯è§†åŒ–æ•°æ®å‡ºé”™ï¼Œå¯ä»¥é€‰æ‹©è®°å½•æ—¥å¿—æˆ–è®¾ç½®é”™è¯¯ä¿¡æ¯
            print(f"æå–æˆ–è§£æå·¥å…·è¾“å‡ºæ—¶å‘ç”Ÿé”™è¯¯: {str(e_inner_parse)}")
            # ä½ å¯èƒ½æƒ³åœ¨è¿™é‡Œä¹Ÿè®¾ç½® error_message

    except TimeoutError as te:
        error_msg_detail = f"æŸ¥è¯¢å¤„ç†è¶…æ—¶: {str(te)}\n{traceback.format_exc()}\n"
        print(error_msg_detail)
        agent_response_output = "æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·å°è¯•ç®€åŒ–æ‚¨çš„è¯·æ±‚æˆ–ä½¿ç”¨æ›´å°çš„æ•°æ®é›†"
        error_message = "è¯·æ±‚è¶…æ—¶"
    except Exception as e_outer:
        error_msg_detail = f"å¤„ç†æœºå™¨å­¦ä¹ æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e_outer)}\n{traceback.format_exc()}\n"
        print(error_msg_detail)
        agent_response_output = f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e_outer)}"
        error_message = str(e_outer)

    # ç»Ÿä¸€çš„è¿”å›ç‚¹
    result = {
        "answer": agent_response_output,
        "visualization_data": visualization_data,
        "table_data": table_data,
        "is_ml_query": is_ml_query_flag
    }
    if error_message:
        result["error"] = error_message
    if error_details:
        result["details"] = error_details
    if expected_format_info:
        result["expected_format"] = expected_format_info

    return result

def enhance_ml_query_with_rag(query, ml_response):
    """ä½¿ç”¨RAGç³»ç»Ÿå¢å¼ºæœºå™¨å­¦ä¹ æŸ¥è¯¢ç»“æœ"""
    from rag_core import query_rag

    # è·å–åŸå§‹MLå›ç­”
    ml_answer = ml_response.get("answer", "")
    return ml_answer  # æˆ–è€…å¢å¼ºåçš„ç»“æœ
